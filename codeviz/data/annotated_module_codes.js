let annotated_module_codes_data = {
    "2388": "Object.defineProperty(exports, \"__esModule\", {\n  value: !0,\n});\nexports.launchSolutions = exports.normalizeCompletionText = undefined;\nconst M_uuid_utils = require(\"uuid-utils\");\nconst M_async_iterable_utils_maybe = require(\"async-iterable-utils\");\nconst M_config_stuff = require(\"config-stuff\");\nconst M_completion_context = require(\"completion-context\");\nconst M_logging_utils = require(\"logging-utils\");\nconst M_openai_conn_utils = require(\"openai_conn_utils\");\nconst M_openai_choices_utils = require(\"openai-choices-utils\");\nconst M_status_reporter_maybe = require(\"status-reporter\");\nconst M_context_extractor_from_identation_maybe = require(\"context-extractor-from-identation-maybe\");\nconst M_prompt_extractor = require(\"prompt-extractor\");\nconst M_get_prompt_parsing_utils_maybe = require(\"get-prompt-parsing-utils\");\nconst M_background_context_provider = require(\"background-context-provider\");\nconst M_postprocess_choice = require(\"postprocess-choice\");\nconst M_telemetry_stuff = require(\"telemetry-stuff\");\nconst M_location_factory = require(\"location-factory\");\nconst logger = new M_logging_utils.Logger(\n  M_logging_utils.LogLevel.INFO,\n  \"solutions\"\n);\nfunction isBlockBodyFinished(e, t, n, r) {\n  return async (o) => {\n    if (r instanceof Array) {\n      const [i, s] = r;\n      return M_context_extractor_from_identation_maybe.isBlockBodyFinishedWithPrefix(\n        e,\n        t,\n        n,\n        o,\n        s\n      );\n    }\n    return M_context_extractor_from_identation_maybe.isBlockBodyFinished(\n      e,\n      t,\n      n,\n      o\n    );\n  };\n}\nasync function solnList(statusReporter, cancellationToken, solnIter) {\n  if (cancellationToken.isCancellationRequested) {\n    statusReporter.removeProgress();\n    return {\n      status: \"FinishedWithError\",\n      error: \"Cancelled\",\n    };\n  }\n  const r = await solnIter.next();\n  return !0 === r.done\n    ? (statusReporter.removeProgress(),\n      {\n        status: \"FinishedNormally\",\n      })\n    : {\n        status: \"Solution\",\n        solution: r.value,\n        next: solnList(statusReporter, cancellationToken, solnIter),\n      };\n}\nexports.normalizeCompletionText = function (e) {\n  return e.replace(/\\s+/g, \"\");\n};\nexports.launchSolutions = async function (ctx, t) {\n  var next;\n  var prev;\n  var w;\n\n  const ctxInsertPos = t.completionContext.insertPosition;\n  const ctxPrependToCompletion = t.completionContext.prependToCompletion;\n  const ctxIndentation = t.completionContext.indentation;\n  const locFactory = ctx.get(M_location_factory.LocationFactory);\n  const doc = await t.getDocument();\n  const k = await M_prompt_extractor.extractPrompt(ctx, doc, ctxInsertPos);\n\n  if (\"contextTooShort\" === k.type) {\n    t.reportCancelled();\n    return {\n      status: \"FinishedWithError\",\n      error: \"Context too short\",\n    };\n  }\n\n  const prompt = k.prompt;\n  const trailingWs = k.trailingWs;\n  if (trailingWs.length > 0) {\n    t.startPosition = locFactory.position(\n      t.startPosition.line,\n      t.startPosition.character - trailingWs.length\n    );\n  }\n\n  const cancellationToken = t.getCancellationToken();\n  const requestId = M_uuid_utils.v4();\n  t.savedTelemetryData = M_telemetry_stuff.TelemetryData.createAndMarkAsIssued(\n    {\n      headerRequestId: requestId,\n      languageId: doc.languageId,\n      source: M_completion_context.completionTypeToString(\n        t.completionContext.completionType\n      ),\n    },\n    {\n      ...M_telemetry_stuff.telemetrizePromptLength(prompt),\n      solutionCount: t.solutionCountTarget,\n      promptEndPos: doc.offsetAt(ctxInsertPos),\n    }\n  );\n\n  if (\n    t.completionContext.completionType ===\n    M_completion_context.CompletionType.TODO_QUICK_FIX\n  ) {\n    const prefixLines = prompt.prefix.split(\"\\n\"),\n      lastLine = prefixLines.pop(),\n      secondLastLine = prefixLines.pop();\n    if (secondLastLine) {\n      const match = /^\\W+(todo:?\\s+)/i.exec(secondLastLine);\n      if (match) {\n        const o = match[1],\n          i = secondLastLine.replace(o, \"\");\n        prompt.prefix = prefixLines.join(\"\\n\") + \"\\n\" + i + \"\\n\" + lastLine;\n      }\n    }\n  }\n\n  if (\n    t.completionContext.completionType ===\n    M_completion_context.CompletionType.UNKNOWN_FUNCTION_QUICK_FIX\n  ) {\n    prompt.prefix += t.completionContext.prependToCompletion;\n  }\n\n  logger.info(ctx, `prompt: ${JSON.stringify(prompt)}`);\n  logger.debug(ctx, `prependToCompletion: ${ctxPrependToCompletion}`);\n\n  M_telemetry_stuff.telemetry(ctx, \"solution.requested\", t.savedTelemetryData);\n\n  const blockMode = await ctx\n    .get(M_config_stuff.BlockModeConfig)\n    .forLanguage(ctx, doc.languageId);\n  const isLangSupported = M_get_prompt_parsing_utils_maybe.isSupportedLanguageId(\n    doc.languageId\n  );\n  const contextIndentation = M_context_extractor_from_identation_maybe.contextIndentation(doc, ctxInsertPos);\n  const postOptions = {\n    stream: !0,\n    extra: {\n      language: doc.languageId,\n      next_indent: null !== (next = contextIndentation.next) && undefined !== next ? next : 0,\n    },\n  };\n  if (\"parsing\" !== blockMode || isLangSupported) {\n    postOptions.stop = [\"\\n\\n\", \"\\r\\n\\r\\n\"];\n  }\n  const repoInfo = M_background_context_provider.extractRepoInfoInBackground(\n    ctx,\n    doc.fileName\n  );\n  const request = {\n    prompt: prompt,\n    languageId: doc.languageId,\n    repoInfo: repoInfo,\n    ourRequestId: requestId,\n    engineUrl: await M_openai_conn_utils.getEngineURL(\n      ctx,\n      M_background_context_provider.tryGetGitHubNWO(repoInfo),\n      doc.languageId,\n      M_background_context_provider.getDogFood(repoInfo),\n      await M_background_context_provider.getUserKind(ctx),\n      t.savedTelemetryData\n    ),\n    count: t.solutionCountTarget,\n    uiKind: M_openai_choices_utils.CopilotUiKind.Panel,\n    postOptions: postOptions,\n    requestLogProbs: !0,\n  };\n  \n  let F;\n  const completionType =\n    t.completionContext.completionType ===\n    M_completion_context.CompletionType.UNKNOWN_FUNCTION_QUICK_FIX\n      ? [\n          M_completion_context.CompletionType.UNKNOWN_FUNCTION_QUICK_FIX,\n          t.completionContext.prependToCompletion,\n        ]\n      : t.completionContext.completionType;\n\n  switch (blockMode) {\n    case M_config_stuff.BlockMode.Server:\n      F = async (e) => {};\n      postOptions.extra.force_indent = null !== (prev = contextIndentation.prev) && undefined !== prev ? prev : -1;\n      postOptions.extra.trim_by_indentation = !0;\n      break;\n    case M_config_stuff.BlockMode.ParsingAndServer:\n      F = isLangSupported ? isBlockBodyFinished(ctx, doc, t.startPosition, completionType) : async (e) => {};\n      postOptions.extra.force_indent = null !== (w = contextIndentation.prev) && undefined !== w ? w : -1;\n      postOptions.extra.trim_by_indentation = !0;\n      break;\n    case M_config_stuff.BlockMode.Parsing:\n    default:\n      F = isLangSupported ? isBlockBodyFinished(ctx, doc, t.startPosition, completionType) : async (e) => {};\n  }\n\n  ctx.get(M_status_reporter_maybe.StatusReporter).setProgress();\n  const response = await ctx\n    .get(M_openai_choices_utils.OpenAIFetcher)\n    .fetchAndStreamCompletions(\n      ctx,\n      request,\n      M_telemetry_stuff.TelemetryData.createAndMarkAsIssued(),\n      F,\n      cancellationToken\n    );\n  \n  if (\"failed\" === response.type || \"canceled\" === response.type) {\n    t.reportCancelled();\n    ctx.get(M_status_reporter_maybe.StatusReporter).removeProgress();\n    return {\n      status: \"FinishedWithError\",\n      error: `${response.type}: ${response.reason}`,\n    };\n  }\n\n  let choices = response.choices;\n  choices = (async function* (choices, prependToCompletion) {\n    for await (const choice of choices) {\n      const choice = {\n        ...choice,\n      };\n      choice.completionText = prependToCompletion + choice.completionText.trimRight();\n      yield choice;\n    }\n  })(choices, ctxPrependToCompletion);\n\n  if (null !== ctxIndentation) {\n    choices = M_openai_choices_utils.cleanupIndentChoices(choices, ctxIndentation);\n  }\n  choices = M_async_iterable_utils_maybe.asyncIterableMapFilter(choices, async (t) =>\n    M_postprocess_choice.postProcessChoice(ctx, \"solution\", doc, ctxInsertPos, t, !1, logger)\n  );\n\n  const choicesProcessor = M_async_iterable_utils_maybe.asyncIterableMapFilter(\n    choices,\n    async (choice) => {\n      let displayText = choice.completionText;\n      logger.info(ctx, `Open Copilot completion: [${choice.completionText}]`);\n\n      if (\n        t.completionContext.completionType ===\n          M_completion_context.CompletionType.OPEN_COPILOT ||\n        t.completionContext.completionType ===\n          M_completion_context.CompletionType.TODO_QUICK_FIX\n      ) {\n        let textBeforeInsertPosSameLine = \"\";\n        const nodeStart = await M_context_extractor_from_identation_maybe.getNodeStart(\n          ctx,\n          doc,\n          ctxInsertPos,\n          choice.completionText\n        );\n        if (nodeStart)\n          [textBeforeInsertPosSameLine] = M_prompt_extractor.trimLastLine(\n            doc.getText(locFactory.range(locFactory.position(nodeStart.line, nodeStart.character), ctxInsertPos))\n          );\n        else {\n          const e = locFactory.position(ctxInsertPos.line, 0);\n          textBeforeInsertPosSameLine = doc.getText(locFactory.range(e, ctxInsertPos));\n        }\n        displayText = textBeforeInsertPosSameLine + displayText;\n      }\n      let completionText = choice.completionText;\n      if (\n        t.completionContext.completionType ===\n        M_completion_context.CompletionType.TODO_QUICK_FIX\n      ) {\n        if (doc.lineAt(ctxInsertPos.line).isEmptyOrWhitespace) {\n          completionText += \"\\n\";\n        }\n      }\n      if (trailingWs.length > 0 && completionText.startsWith(trailingWs)) {\n        completionText = completionText.substring(trailingWs.length);\n      }\n      const meanLogProb = choice.meanLogProb;\n      return {\n        displayText: displayText,\n        meanProb: undefined !== meanLogProb ? Math.exp(meanLogProb) : 0,\n        meanLogProb: meanLogProb || 0,\n        completionText: completionText,\n        requestId: choice.requestId,\n        choiceIndex: choice.choiceIndex,\n        prependToCompletion: ctxPrependToCompletion,\n      };\n    }\n  );\n\n  return solnList(\n    ctx.get(M_status_reporter_maybe.StatusReporter),\n    cancellationToken,\n    choicesProcessor[Symbol.asyncIterator]()\n  );\n};",
    "3197": "Object.defineProperty(exports, \"__esModule\", {\n    value: !0,\n  });\n  exports.registerGhostText =\n    exports.handleGhostTextPostInsert =\n    exports.handleGhostTextShown =\n    exports.provideInlineCompletions =\n    exports.ghostTextLogger =\n    exports.getInsertionTextFromCompletion =\n      undefined;\n  const M_vscode = require(\"vscode\");\n  const M_config_stuff = require(\"config-stuff\");\n  const M_completion_from_ghost_text = require(\"completion-from-ghost-text\");\n  const M_ghost_text_provider = require(\"ghost-text-provider\");\n  const M_ghost_text_telemetry = require(\"ghost-text-telemetry\");\n  const M_logging_utils = require(\"logging-utils\");\n  const M_post_accept_or_reject_tasks = require(\"post-accept-or-reject-tasks\");\n  const M_telemetry_stuff = require(\"telemetry-stuff\");\n  const M_ignore_document_or_not = require(\"ignore-document-or-not\");\n  \n  // https://code.visualstudio.com/api/references/vscode-api#InlineCompletionItem\n  const postInsertCmdName = \"_ghostTextPostInsert\"; // this command is called after an item is inserted\n  \n  function getInsertionTextFromCompletion(e) {\n    return e.insertText;\n  }\n  \n  // `f` and `m` seem to store state across suggestions\n  // unable to completely decipher how they're updated, but\n  // f seems to store positions, and m seems to store uri of the\n  // document where the completion was shown.\n  let f;\n  let m;\n  \n  exports.getInsertionTextFromCompletion = getInsertionTextFromCompletion;\n  exports.ghostTextLogger = new M_logging_utils.Logger(\n    M_logging_utils.LogLevel.INFO,\n    \"ghostText\"\n  );\n  let shownItemIdx;\n  let shownItems = [];\n  // this.ctx, doc, pos, completionCtx, cancellationToken\n  async function provideInlineCompletions(ctx, doc, pos, completionCtx, cancellationToken) {\n    const fn = await (async function (ctx, doc, pos, completionCtx, cancellationToken) {\n      const telemetryData = M_telemetry_stuff.TelemetryData.createAndMarkAsIssued();\n      // check if inline suggestions are even enabled.\n      if (\n        !(function (ctx) {\n          return M_config_stuff.getConfig(\n            ctx,\n            M_config_stuff.ConfigKey.InlineSuggestEnable\n          );\n        })(ctx)\n      )\n        return {\n          type: \"abortedBeforeIssued\",\n          reason: \"ghost text is disabled\",\n        };\n      \n      // check if doc should be ignored\n      if (M_ignore_document_or_not.ignoreDocument(ctx, doc))\n        return {\n          type: \"abortedBeforeIssued\",\n          reason: \"document is ignored\",\n        };\n      \n      exports.ghostTextLogger.debug(\n        ctx,\n        `Ghost text called at [${pos.line}, ${pos.character}], with triggerKind ${completionCtx.triggerKind}`\n      );\n  \n      // don't proceed if cancelled already\n      if (cancellationToken.isCancellationRequested)\n        return (\n          exports.ghostTextLogger.info(ctx, \"Cancelled before extractPrompt\"),\n          {\n            type: \"abortedBeforeIssued\",\n            reason: \"cancelled before extractPrompt\",\n          }\n        );\n      \n      // nice, I didn't think of this while using copilot\n      if (completionCtx.selectedCompletionInfo) {\n        exports.ghostTextLogger.debug(\n          ctx,\n          \"Not showing ghost text because autocomplete widget is displayed\"\n        );\n        return {\n          type: \"abortedBeforeIssued\",\n          reason: \"autocomplete widget is displayed\",\n        };\n      }\n  \n      // the actual work happens here\n      const ghostTextResult = await M_ghost_text_provider.getGhostText(\n        ctx,\n        doc,\n        pos,\n        completionCtx.triggerKind === M_vscode.InlineCompletionTriggerKind.Invoke,\n        telemetryData,\n        cancellationToken\n      );\n  \n      if (\"success\" !== ghostTextResult.type) {\n        exports.ghostTextLogger.debug(\n          ctx,\n          \"Breaking, no results from getGhostText -- \" + ghostTextResult.type + \": \" + ghostTextResult.reason\n        );\n        return ghostTextResult;\n      }\n  \n      // this b,w is hard to decipher. Need to get back to this.\n      const [b, w] = ghostTextResult.value;\n      if (\n        f &&\n        m &&\n        (!f.isEqual(pos) || m !== doc.uri) &&\n        w !== M_ghost_text_provider.ResultType.TypingAsSuggested\n      ) {\n        // i THINK what's happening here is if someone continues typing after seeing a suggestion\n        // but the typing differs from the suggestion, then some stuff gets\n        // sent to telemetry. Will get back to this.\n        // my guess is _ contains stuff that's been typed so far....i think???????\n        const t = shownItems.flatMap((shownItem) =>\n          shownItem.displayText && shownItem.telemetry\n            ? [\n                {\n                  completionText: shownItem.displayText,\n                  completionTelemetryData: shownItem.telemetry,\n                },\n              ]\n            : []\n        );\n        if (t.length > 0) {\n          M_post_accept_or_reject_tasks.postRejectionTasks(\n            ctx,\n            \"ghostText\",\n            doc.offsetAt(f),\n            m,\n            t\n          );\n        }\n      }\n      f = pos;\n      m = doc.uri;\n      shownItems = [];\n  \n      if (cancellationToken.isCancellationRequested)\n        return (\n          exports.ghostTextLogger.info(ctx, \"Cancelled after getGhostText\"),\n          {\n            type: \"canceled\",\n            reason: \"after getGhostText\",\n            telemetryData: {\n              telemetryBlob: ghostTextResult.telemetryBlob,\n            },\n          }\n        );\n      \n      \n      const completions = M_completion_from_ghost_text.completionsFromGhostTextResults(\n        ctx,\n        b,\n        w,\n        doc,\n        pos,\n        (function (e) {\n          const t = M_vscode.window.visibleTextEditors.find(\n            (t) => t.document === e\n          );\n          return null == t ? undefined : t.options;\n        })(doc),\n        shownItemIdx\n      );\n  \n      exports.ghostTextLogger.debug(ctx, \"Completions\", completions);\n      \n      const inlineCompletionItems = completions.map((completion) => {\n        const { text: t, range: o } = completion;\n        const i = new M_vscode.Range(\n          new M_vscode.Position(o.start.line, o.start.character),\n          new M_vscode.Position(o.end.line, o.end.character)\n        );\n        const item = new M_vscode.InlineCompletionItem(t, i);\n        item.index = completion.index;\n        item.telemetry = completion.telemetry;\n        item.displayText = completion.displayText;\n        item.resultType = completion.resultType;\n        item.uri = doc.uri;\n        item.insertOffset = doc.offsetAt(\n          new M_vscode.Position(completion.position.line, completion.position.character)\n        );\n        // after this item is inserted, the `handleGhostTextPostInsert` will get called\n        // because that's the fn registered as callback for `postInsertCmdName`\n        item.command = {\n          title: \"PostInsertTask\",\n          command: postInsertCmdName,\n          arguments: [item],\n        };\n        return item;\n      });\n  \n      return 0 === inlineCompletionItems.length\n        ? {\n            type: \"empty\",\n            reason: \"no completions in final result\",\n            telemetryData: ghostTextResult.telemetryData,\n          }\n        : {\n            ...ghostTextResult,\n            value: inlineCompletionItems,\n          };\n    })(ctx, doc, pos, completionCtx, cancellationToken);\n    return await M_ghost_text_telemetry.handleGhostTextResultTelemetry(ctx, fn);\n  }\n  exports.provideInlineCompletions = provideInlineCompletions;\n  \n  // implements vscode.InlineCompletionItemProvider\n  class InlineCompletionItemProvider {\n    constructor(e) {\n      this.ctx = e;\n    }\n    // provideInlineCompletionItems(document: TextDocument, position: Position, context: InlineCompletionContext, token: CancellationToken): ProviderResult<InlineCompletionList<T> | T[]>;\n    async provideInlineCompletionItems(doc, pos, completionCtx, cancellationToken) {\n      return provideInlineCompletions(this.ctx, doc, pos, completionCtx, cancellationToken);\n    }\n    // undocumented function that gets called by vscode (whenever an item is shown?)\n    // found info here: https://github.com/microsoft/vscode/issues/153754\n    // https://github.com/juihanamshet1/HandleDidShowCompletionItem-bug/blob/9a48d06b3032b2b5dccc83aedbafeda7fa6d3786/inline-completions/src/extension.ts#L56\n    handleDidShowCompletionItem(item) { // item is of type vscode.InlineCompletionItem\n      handleGhostTextShown(this.ctx, item);\n    }\n  }\n  \n  function handleGhostTextShown(ctx, shownItem) {\n    shownItemIdx = shownItem.index;\n    if (!shownItems.find((e) => e.index === shownItem.index) &&\n          (shownItems.push(shownItem), shownItem.telemetry)) {\n      const fromCache = !(shownItem.resultType === M_ghost_text_provider.ResultType.Network);\n      \n      exports.ghostTextLogger.debug(\n        ctx,\n        `[${shownItem.telemetry.properties.headerRequestId}] shown choiceIndex: ${shownItem.telemetry.properties.choiceIndex}, fromCache ${fromCache}`\n      );\n  \n        // record which item was shown to you.\n        M_ghost_text_telemetry.telemetryShown(\n          ctx,\n          \"ghostText\",\n          shownItem.telemetry,\n          fromCache\n        );\n    }\n  }\n  \n  // this function is called after an item is inserted\n  async function handleGhostTextPostInsert(ctx, item) {\n    // reset state variables.\n    shownItems = [];\n    m = undefined;\n    f = undefined;\n    exports.ghostTextLogger.debug(ctx, \"Ghost text post insert\");\n    if (\n      item.telemetry &&\n      item.uri &&\n      item.displayText &&\n      undefined !== item.insertOffset &&\n      item.range\n    ) {\n      // record that the item was inserted\n      item.telemetry.measurements.compCharLen =\n        getInsertionTextFromCompletion(item).length;\n      await M_post_accept_or_reject_tasks.postInsertionTasks(\n        ctx,\n        \"ghostText\",\n        item.displayText,\n        item.insertOffset,\n        item.uri,\n        item.telemetry\n      );\n    }\n  }\n  \n  exports.handleGhostTextShown = handleGhostTextShown;\n  exports.handleGhostTextPostInsert = handleGhostTextPostInsert;\n  exports.registerGhostText = function (ctx) {\n    const t = new InlineCompletionItemProvider(ctx);\n    return [\n      M_vscode.languages.registerInlineCompletionItemProvider(\n        {\n          pattern: \"**\",\n        },\n        t\n      ),\n      M_vscode.commands.registerCommand(postInsertCmdName, async (item) =>\n        handleGhostTextPostInsert(ctx, item)\n      ),\n    ];\n  };\n  ",
    "4969": "// prompt-extractor.js\nObject.defineProperty(exports, \"__esModule\", {\n  value: !0,\n});\nexports.extractPrompt =\n  exports.trimLastLine =\n  exports._contextTooShort =\n  exports.MIN_PROMPT_CHARS =\n    undefined;\n\nconst M_getPrompt_main_stuff = require(\"getPrompt-main-stuff\");\nconst M_config_stuff = require(\"config-stuff\");\nconst M_doc_tracker = require(\"doc-tracker\");\nconst M_task_maybe = require(\"task\");\nconst M_text_doc_relative_path = require(\"text-doc-relative-path\");\nconst M_get_prompt_parsing_utils_maybe = require(\"get-prompt-parsing-utils\");\nconst M_background_context_provider = require(\"background-context-provider\");\n\nfunction trimLastLine(str) { // returns [trimmedString, ws]\n  const lines = str.split(\"\\n\");\n  const lastLine = lines[lines.length - 1];\n  const nTrailingWS = lastLine.length - lastLine.trimRight().length;\n  const beforeWS = str.slice(0, str.length - nTrailingWS);\n  const ws = str.substr(beforeWS.length);\n  return [lastLine.length == nTrailingWS ? beforeWS : str, ws];\n}\n\n// (ctx, doc.getText(), doc.offsetAt(insertPos), relativePath, doc.uri, doc.languageId)\nasync function getPromptHelper(ctx, docText, insertOffset, docRelPath, docUri, docLangId) {\n  var githubNWORaw; // NWO = \"name with owner\", e.g., \"microsoft/vscode\"\n  const githubNWO =\n    null !==\n      (githubNWORaw = M_background_context_provider.tryGetGitHubNWO(\n        M_background_context_provider.extractRepoInfoInBackground(ctx, docUri.fsPath)\n      )) && undefined !== githubNWORaw\n      ? githubNWORaw\n      : \"\";\n  \n  const suffixPercent = await M_config_stuff.suffixPercent(ctx, githubNWO, docLangId);\n  const fimSuffixLengthThresh = await M_config_stuff.fimSuffixLengthThreshold(ctx, githubNWO, docLangId);\n  // if suffixPercent > 0, then we're in FIM mode, which means the context can be the whole file\n  // otherwise, it's everything till the cursor.\n  // context size is determined based on the above two conditions.\n  if ((suffixPercent > 0 ? docText.length : insertOffset) < exports.MIN_PROMPT_CHARS)\n    return exports._contextTooShort;\n  \n  const now = Date.now();\n  const {\n    prefix: prefix,\n    suffix: suffix,\n    promptChoices: promptChoices,\n    promptBackground: promptBackground,\n    promptElementRanges: promptElementRanges,\n  } = await (async function (ctx, docText, insertOffset, docRelPath, docUri, docLangId) {\n    var h;\n    let relevantDocs = []; // list of atmost 20 other files in the workspace that are of the same language as the current file\n    relevantDocs = await (async function (ctx, docFsPath, docLangId) {\n      // stores a list of {uri, relativePath, languageId, source} for all OTHER files in the workspace\n      // that are of the same language as the current file\n      const relevantDocs = [];\n\n      // sortedTextDocs is a sorted array of all \"text docs known to the editor\"\n      // https://code.visualstudio.com/api/references/vscode-api\n      const sortedTextDocs = M_doc_tracker.sortByAccessTimes(\n        ctx.get(M_text_doc_relative_path.TextDocumentManager).textDocuments\n      );\n\n      let totalSize = 0;\n      for (const doc of sortedTextDocs) {\n        // if we've already added 20 files, or the total size of all files is > 200k, stop\n        if (relevantDocs.length + 1 > 20 || totalSize + doc.getText().length > 2e5) break;\n\n        if (\"file\" == doc.uri.scheme && doc.fileName !== docFsPath && doc.languageId === docLangId) {\n          relevantDocs.push({\n            uri: doc.uri.toString(),\n            relativePath: await ctx\n              .get(M_text_doc_relative_path.TextDocumentManager)\n              .getRelativePath(doc), // takes care of edge cases like \"Untitled-1\"\n            languageId: doc.languageId,\n            source: doc.getText(),\n          });\n        \n          totalSize += doc.getText().length;\n        }\n      }\n      return relevantDocs;\n    })(ctx, docUri.fsPath, docLangId);\n    \n    const thisFile = {\n      uri: docUri.toString(),\n      source: docText,\n      offset: insertOffset,\n      relativePath: docRelPath,\n      languageId: docLangId,\n    };\n\n    const githubNWO =\n      null !==\n        (h = M_background_context_provider.tryGetGitHubNWO(\n          M_background_context_provider.extractRepoInfoInBackground(ctx, docUri.fsPath)\n        )) && undefined !== h\n        ? h\n        : \"\";\n\n    let promptOptions = {\n      // copilot still uses contextSize = 2048\n      maxPromptLength:\n        2048 -\n        M_config_stuff.getConfig(ctx, M_config_stuff.ConfigKey.SolutionLength),\n\n      neighboringTabs: await ctx\n        .get(M_task_maybe.Features)\n        .neighboringTabsOption(githubNWO, docLangId),\n\n      // one of Cursor, CursorTrimStart, SiblingBlock, SiblingBlockTrimStart\n      // (getPrompt-main-stuff)\n      suffixStartMode: await ctx.get(M_task_maybe.Features).suffixStartMode(githubNWO, docLangId),\n    };\n\n    const suffixPercent = await M_config_stuff.suffixPercent(ctx, githubNWO, docLangId);\n    const suffixMatchThresh = await M_config_stuff.suffixMatchThreshold(ctx, githubNWO, docLangId);\n    const fimSuffixLengthThresh = await M_config_stuff.fimSuffixLengthThreshold(ctx, githubNWO, docLangId);\n\n    if (suffixPercent > 0) {\n      promptOptions = {\n        ...promptOptions,\n        // huh, this one's hardcoded in this version.\n        includeSiblingFunctions:\n          M_getPrompt_main_stuff.SiblingOption.NoSiblings,\n\n        suffixPercent: suffixPercent,\n        suffixMatchThreshold: suffixMatchThresh,\n        fimSuffixLengthThreshold: fimSuffixLengthThresh,\n      };\n    }\n    const fs = ctx.get(M_getPrompt_main_stuff.FileSystem);\n    return await M_get_prompt_parsing_utils_maybe.getPrompt(fs, thisFile, promptOptions, relevantDocs);\n  })(ctx, docText, insertOffset, docRelPath, docUri, docLangId);\n\n  const [trimmedPrefix, trailingWs] = trimLastLine(prefix);\n  const now2 = Date.now();\n\n  return {\n    type: \"prompt\",\n    prompt: {\n      prefix: trimmedPrefix,\n      suffix: suffix,\n      isFimEnabled: suffixPercent > 0 && suffix.length > fimSuffixLengthThresh,\n      promptElementRanges: promptElementRanges.ranges,\n    },\n    trailingWs: trailingWs,\n    promptChoices: promptChoices,\n    computeTimeMs: now2 - now,\n    promptBackground: promptBackground,\n  };\n}\n\nasync function getPromptForRegularDoc(ctx, doc, insertPos) {\n  const relativePath = await ctx\n    .get(M_text_doc_relative_path.TextDocumentManager)\n    .getRelativePath(doc);\n  return getPromptHelper(ctx, doc.getText(), doc.offsetAt(insertPos), relativePath, doc.uri, doc.languageId);\n}\n\nexports.MIN_PROMPT_CHARS = 10;\nexports._contextTooShort = {\n  type: \"contextTooShort\",\n};\n\nexports.trimLastLine = trimLastLine;\n\nexports.extractPrompt = function (ctx, doc, insertPos) {\n  const nb = ctx.get(M_text_doc_relative_path.TextDocumentManager).findNotebook(doc);\n  return undefined === nb\n    ? getPromptForRegularDoc(ctx, doc, insertPos)\n    : (async function (ctx, doc, nb, insertPos) {\n        const theCell = nb.getCells().find((e) => e.document.uri === doc.uri);\n        if (theCell) {\n          const beforeCells = nb\n            .getCells()\n            .filter(\n              (e) =>\n                e.index < theCell.index &&\n                e.document.languageId === theCell.document.languageId\n            );\n          const beforeCellsCatted =\n            beforeCells.length > 0\n              ? beforeCells.map((e) => e.document.getText()).join(\"\\n\\n\") + \"\\n\\n\"\n              : \"\";\n          // wait won't this variable contain duplicate code if doc.getText returns all code?\n          // idk the notebook api\n          const codeTillThisCell = beforeCellsCatted + doc.getText();\n          const l = beforeCellsCatted.length + doc.offsetAt(insertPos);\n          const u = await ctx\n            .get(M_text_doc_relative_path.TextDocumentManager)\n            .getRelativePath(doc);\n          return getPromptHelper(ctx, codeTillThisCell, l, u, doc.uri, theCell.document.languageId);\n        }\n        return getPromptForRegularDoc(ctx, doc, insertPos);\n      })(ctx, doc, nb, insertPos);\n};\n",
    "7017": "Object.defineProperty(exports, \"__esModule\", {\n    value: true,\n  });\n  exports.postInsertionTasks =\n    exports.postRejectionTasks =\n    exports.captureCode =\n      undefined;\n  const M_change_tracker = require(\"change-tracker\");\n  const M_ghost_text_telemetry = require(\"ghost-text-telemetry\");\n  const M_logging_utils = require(\"logging-utils\");\n  const M_context_extractor_from_identation_maybe = require(\"context-extractor-from-identation-maybe\");\n  const M_prompt_extractor = require(\"prompt-extractor\");\n  const M_edit_distance_maybe = require(\"edit-distance\");\n  const M_telemetry_stuff = require(\"telemetry-stuff\");\n  const M_text_doc_relative_path = require(\"text-doc-relative-path\");\n  const d = new M_logging_utils.Logger(\n    M_logging_utils.LogLevel.INFO,\n    \"post-insertion\"\n  );\n  // These appear to be different timestamps when the measurements are taken.\n  const measurementConfigs = [\n    {\n      seconds: 15,\n      captureCode: false,\n      captureRejection: false,\n    },\n    {\n      seconds: 30,\n      captureCode: true,\n      captureRejection: true,\n    },\n    {\n      seconds: 120,\n      captureCode: false,\n      captureRejection: false,\n    },\n    {\n      seconds: 300,\n      captureCode: false,\n      captureRejection: false,\n    },\n    {\n      seconds: 600,\n      captureCode: false,\n      captureRejection: false,\n    },\n  ];\n  // ctx, docUri, chngTracker.offset\n  async function captureCode(ctx, docUri, insertionOffset) {\n    const doc = await ctx\n      .get(M_text_doc_relative_path.TextDocumentManager)\n      .getTextDocument(docUri);\n    if (!doc) {\n      d.info(\n        ctx,\n        `Could not get document for ${docUri.fsPath}. Maybe it was closed by the editor.`\n      );\n      return {\n        prompt: {\n          prefix: \"\",\n          suffix: \"\",\n          isFimEnabled: false,\n          promptElementRanges: [],\n        },\n        capturedCode: \"\",\n        terminationOffset: 0,\n      };\n    }\n    const docText = doc.getText();\n    const prefix = docText.substring(0, insertionOffset);\n    const insertionPos = doc.positionAt(insertionOffset);\n    // huh - you extract the prompt afresh? interesting. very interesting.\n    // so now, instead of \"remembering\" what the original prompt was\n    // and therefore \"marking\" that prompt lead/didn't lead to correct\n    // completion.......instead of doing that, you're just collecting\n    // new training data -- you're saying that at this inspection timestamp,\n    // extract prompt that'd be used to complete at the insertion position\n    // and we now already have ground truth of this insertion position\n    // (stable version of the code after T seconds.)\n    //\n    // Interesting idea, except this capture-code method gets called\n    // just 30s after accept/reject. So...chances are that it was done\n    // this way just to not remember the prompt? Who knows.\n    const promptWrapper = await M_prompt_extractor.extractPrompt(ctx, doc, insertionPos);\n    const prompt =\n      \"prompt\" === promptWrapper.type\n        ? promptWrapper.prompt\n        : {\n            prefix: prefix,\n            suffix: \"\",\n            isFimEnabled: false,\n            promptElementRanges: [],\n          };\n    const suffixAfterInsertion = docText.substring(insertionOffset);\n    // absolutely no clue what this context-indentation-from-text thing is.\n    // what does the name even mean.\n    const f =\n      M_context_extractor_from_identation_maybe.contextIndentationFromText(\n        prefix,\n        insertionOffset,\n        doc.languageId\n      );\n    const m = M_context_extractor_from_identation_maybe.indentationBlockFinished(\n      f,\n      undefined\n    );\n    const g = await m(suffixAfterInsertion);\n    const _ = Math.min(docText.length, insertionOffset + (g ? 2 * g : 500));\n    // but basically looks like it's used to determine how much of the code\n    // after the insertion point is relevant for this code completion.\n    // because capturedCode is docText.substring(insertionOffset, _)\n    return {\n      prompt: prompt,\n      capturedCode: docText.substring(insertionOffset, _),\n      terminationOffset: null != g ? g : -1,\n    };\n  }\n  // docText, displayText, 50, chngTracker.offset\n  // docText, displayText, 1500, chngTracker.offset\n  // This function appears to compute edit distance between\n  // a window around where the suggestion was inserted,\n  // and the suggestion itself.\n  // not sure why window is considered here exactly.\n  // window size is 50 and 1500 resp.\n  function f(docText, choiceDisplayText, n, offset) {\n    const windowAroundInsertion = docText.substring(\n      Math.max(0, offset - n),\n      Math.min(docText.length, offset + choiceDisplayText.length + n)\n    );\n    const i = M_edit_distance_maybe.lexEditDistance(windowAroundInsertion, choiceDisplayText);\n    const s = i.lexDistance / i.needleLexLength;\n    const { distance: a } = M_edit_distance_maybe.editDistance(\n      windowAroundInsertion.substring(i.startOffset, i.endOffset),\n      choiceDisplayText\n    );\n    return {\n      relativeLexEditDistance: s,\n      charEditDistance: a,\n      completionLexLength: i.needleLexLength,\n      foundOffset: i.startOffset + Math.max(0, offset - n),\n      lexEditDistance: i.lexDistance,\n      stillInCodeHeuristic: s <= 0.5 ? 1 : 0,\n    };\n  }\n  exports.captureCode = captureCode;\n  exports.postRejectionTasks = function (e, t, n, i, s) {\n    s.forEach(({ completionText: n, completionTelemetryData: r }) => {\n      d.debug(e, `${t}.rejected choiceIndex: ${r.properties.choiceIndex}`);\n      M_ghost_text_telemetry.telemetryRejected(e, t, r);\n    });\n    const a = new M_change_tracker.ChangeTracker(e, i, n);\n    measurementConfigs.filter((e) => e.captureRejection).map((r) => {\n      a.push(async () => {\n        d.debug(e, `Original offset: ${n}, Tracked offset: ${a.offset}`);\n        const { completionTelemetryData: o } = s[0];\n        const {\n          prompt: c,\n          capturedCode: u,\n          terminationOffset: p,\n        } = await captureCode(e, i, a.offset);\n        let f;\n        f = c.isFimEnabled\n          ? {\n              hypotheticalPromptPrefixJson: JSON.stringify(c.prefix),\n              hypotheticalPromptSuffixJson: JSON.stringify(c.suffix),\n            }\n          : {\n              hypotheticalPromptJson: JSON.stringify(c.prefix),\n            };\n        const m = o.extendedBy(\n          {\n            ...f,\n            capturedCodeJson: JSON.stringify(u),\n          },\n          {\n            timeout: r.seconds,\n            insertionOffset: n,\n            trackedOffset: a.offset,\n            terminationOffsetInCapturedCode: p,\n          }\n        );\n        d.debug(\n          e,\n          `${t}.capturedAfterRejected choiceIndex: ${o.properties.choiceIndex}`,\n          m\n        );\n        M_telemetry_stuff.telemetry(e, t + \".capturedAfterRejected\", m, true);\n      }, 1e3 * r.seconds);\n    });\n  };\n  \n  // ctx, \"ghostText\", item.displayText, item.insertOffset, item.uri, item.telemetry\n  exports.postInsertionTasks = async function (ctx, flow, choiceDisplayText, insertOffset, docUri, telemetry) {\n    d.debug(ctx, `${flow}.accepted choiceIndex: ${telemetry.properties.choiceIndex}`);\n    M_ghost_text_telemetry.telemetryAccepted(ctx, flow, telemetry);\n    const chngTracker = new M_change_tracker.ChangeTracker(ctx, docUri, insertOffset);\n    const displayText = choiceDisplayText.trim();\n    // set a timeout for every measurement config shown above\n    // i.e., after 15s, 30s, 2min, 5min, 10min\n    measurementConfigs.map((mConf) =>\n      chngTracker.push(\n        () =>\n          (async function (ctx, flow, displayText, insertOffset, docUri, mConf, telemetry, chngTracker) {\n            const doc = await ctx\n              .get(M_text_doc_relative_path.TextDocumentManager)\n              .getTextDocument(docUri);\n            if (doc) {\n              const docText = doc.getText();\n              let p = f(docText, displayText, 50, chngTracker.offset);\n              if (p.stillInCodeHeuristic) {\n                p = f(docText, displayText, 1500, chngTracker.offset);\n              }\n              d.debug(\n                ctx,\n                `stillInCode: ${\n                  p.stillInCodeHeuristic ? \"Found\" : \"Not found\"\n                }! Completion '${displayText}' in file ${\n                  docUri.fsPath\n                }. lexEditDistance fraction was ${\n                  p.relativeLexEditDistance\n                }. Char edit distance was ${\n                  p.charEditDistance\n                }. Inserted at ${insertOffset}, tracked at ${chngTracker.offset}, found at ${\n                  p.foundOffset\n                }. choiceIndex: ${telemetry.properties.choiceIndex}`\n              );\n              const m = telemetry\n                .extendedBy(\n                  {},\n                  {\n                    timeout: mConf.seconds,\n                    insertionOffset: insertOffset,\n                    trackedOffset: chngTracker.offset,\n                  }\n                )\n                .extendedBy({}, p);\n              M_telemetry_stuff.telemetry(ctx, flow + \".stillInCode\", m);\n              if (mConf.captureCode) {\n                const {\n                  prompt: prompt,\n                  capturedCode: c,\n                  terminationOffset: u,\n                } = await captureCode(ctx, docUri, chngTracker.offset);\n                let p;\n                p = prompt.isFimEnabled\n                  ? {\n                      hypotheticalPromptPrefixJson: JSON.stringify(prompt.prefix),\n                      hypotheticalPromptSuffixJson: JSON.stringify(prompt.suffix),\n                    }\n                  : {\n                      hypotheticalPromptJson: JSON.stringify(prompt.prefix),\n                    };\n                const f = telemetry.extendedBy(\n                  {\n                    ...p,\n                    capturedCodeJson: JSON.stringify(c),\n                  },\n                  {\n                    timeout: mConf.seconds,\n                    insertionOffset: insertOffset,\n                    trackedOffset: chngTracker.offset,\n                    terminationOffsetInCapturedCode: u,\n                  }\n                );\n                d.debug(\n                  ctx,\n                  `${flow}.capturedAfterAccepted choiceIndex: ${telemetry.properties.choiceIndex}`,\n                  m\n                ),\n                  (0, M_telemetry_stuff.telemetry)(\n                    ctx,\n                    flow + \".capturedAfterAccepted\",\n                    f,\n                    true\n                  );\n              }\n            }\n          })(ctx, flow, displayText, insertOffset, docUri, mConf, telemetry, chngTracker),\n        1e3 * mConf.seconds\n      )\n    );\n  };\n  ",
    "9334": "Object.defineProperty(exports, \"__esModule\", {\n    value: !0,\n  });\n  \n  exports.getGhostText =\n    exports.completionCache =\n    exports.ResultType =\n    exports.ghostTextLogger =\n      undefined;\n  \n  const M_getPrompt_main_stuff = require(\"getPrompt-main-stuff\");\n  const M_uuid_utils = require(\"uuid-utils\");\n  const M_prompt_cache = require(\"prompt-cache\");\n  const M_debouncer = require(\"debouncer\");\n  const M_async_iterable_utils_maybe = require(\"async-iterable-utils\");\n  const M_config_stuff = require(\"config-stuff\");\n  const M_task_maybe = require(\"task\");\n  const M_logging_utils = require(\"logging-utils\");\n  const M_helix_fetcher_and_network_stuff = require(\"helix-fetcher-and-network-stuff\");\n  const M_openai_conn_utils = require(\"openai_conn_utils\");\n  const M_live_openai_fetcher = require(\"live-openai-fetcher\");\n  const M_openai_choices_utils = require(\"openai-choices-utils\");\n  const M_status_reporter_maybe = require(\"status-reporter\");\n  const M_context_extractor_from_identation_maybe = require(\"context-extractor-from-identation-maybe\");\n  const M_prompt_extractor = require(\"prompt-extractor\");\n  const M_background_context_provider = require(\"background-context-provider\");\n  const M_ghost_text_score_maybe = require(\"ghost-text-score\");\n  const M_postprocess_choice = require(\"postprocess-choice\");\n  const M_telemetry_stuff = require(\"telemetry-stuff\");\n  const M_runtime_mode_maybe = require(\"runtime-mode\");\n  const M_location_factory = require(\"location-factory\");\n  const M_contextual_filter_manager = require(\"contextual-filter-manager\");\n  const M_ghost_text_debouncer_maybe = require(\"ghost-text-debouncer\");\n  const M_ghost_text_telemetry = require(\"ghost-text-telemetry\");\n  \n  var ResultType;\n  \n  // seems to be like some state variables\n  // not fully understood them but it seems like I is used\n  // for storing doc from start till the cursor position\n  // that's used for detecting if user is \"TypingAsSuggested\"\n  // P is storing the last prompt's cache key.\n  // but my understanding is murky.\n  let I;\n  let P;\n  \n  async function A(e, n, r, o, i, s, a) {\n    var u;\n    var p;\n    var m;\n    exports.ghostTextLogger.debug(e, `Getting ${s} from network`);\n    r = r.extendedBy();\n    const g = await (async function (e, t) {\n      const n = await e.get(M_task_maybe.Features).overrideNumGhostCompletions();\n      return n\n        ? t.isCycling\n          ? Math.max(0, 3 - n)\n          : n\n        : M_config_stuff.shouldDoParsingTrimming(t.blockMode) && t.multiline\n        ? M_config_stuff.getConfig(e, M_config_stuff.ConfigKey.InlineSuggestCount)\n        : t.isCycling\n        ? 2\n        : 1;\n    })(e, n);\n    const _ = M_openai_choices_utils.getTemperatureForSamples(e, g);\n    const y = {\n      stream: !0,\n      n: g,\n      temperature: _,\n      extra: {\n        language: n.languageId,\n        next_indent: null !== (u = n.indentation.next) && undefined !== u ? u : 0,\n        trim_by_indentation: M_config_stuff.shouldDoServerTrimming(n.blockMode),\n      },\n    };\n    if (n.multiline) {\n      y.stop = [\"\\n\"];\n    }\n    if (n.multiline && n.multiLogitBias) {\n      y.logit_bias = {\n        50256: -100,\n      };\n    }\n    const v = Date.now();\n    const b = {\n      endpoint: \"completions\",\n      uiKind: M_live_openai_fetcher.CopilotUiKind.GhostText,\n      isCycling: JSON.stringify(n.isCycling),\n      temperature: JSON.stringify(_),\n      n: JSON.stringify(g),\n      stop:\n        null !== (p = JSON.stringify(y.stop)) && undefined !== p ? p : \"unset\",\n      logit_bias: JSON.stringify(\n        null !== (m = y.logit_bias) && undefined !== m ? m : null\n      ),\n    };\n    const E = M_telemetry_stuff.telemetrizePromptLength(n.prompt);\n    Object.assign(r.properties, b);\n    Object.assign(r.measurements, E);\n    try {\n      const s = {\n        prompt: n.prompt,\n        languageId: n.languageId,\n        repoInfo: n.repoInfo,\n        ourRequestId: n.ourRequestId,\n        engineUrl: n.engineURL,\n        count: g,\n        uiKind: M_live_openai_fetcher.CopilotUiKind.GhostText,\n        postOptions: y,\n      };\n      if (n.delayMs > 0) {\n        await new Promise((e) => setTimeout(e, n.delayMs));\n      }\n      const c = await e\n        .get(M_live_openai_fetcher.OpenAIFetcher)\n        .fetchAndStreamCompletions(e, s, r, i, o);\n      return \"failed\" === c.type\n        ? {\n            type: \"failed\",\n            reason: c.reason,\n            telemetryData: M_ghost_text_telemetry.mkBasicResultTelemetry(r),\n          }\n        : \"canceled\" === c.type\n        ? (exports.ghostTextLogger.debug(\n            e,\n            \"Cancelled after awaiting fetchCompletions\"\n          ),\n          {\n            type: \"canceled\",\n            reason: c.reason,\n            telemetryData: M_ghost_text_telemetry.mkCanceledResultTelemetry(r),\n          })\n        : a(g, v, c.getProcessingTime(), c.choices);\n    } catch (n) {\n      if (M_helix_fetcher_and_network_stuff.isAbortError(n))\n        return {\n          type: \"canceled\",\n          reason: \"network request aborted\",\n          telemetryData: M_ghost_text_telemetry.mkCanceledResultTelemetry(r, {\n            cancelledNetworkRequest: !0,\n          }),\n        };\n      exports.ghostTextLogger.error(e, `Error on ghost text request ${n}`);\n      if ((0, M_runtime_mode_maybe.shouldFailForDebugPurposes)(e)) throw n;\n      return {\n        type: \"failed\",\n        reason: \"non-abort error on ghost text request\",\n        telemetryData: M_ghost_text_telemetry.mkBasicResultTelemetry(r),\n      };\n    }\n  }\n  \n  function trimCompletion(choice, opt) {\n    const n = {\n      ...choice,\n    };\n    n.completionText = choice.completionText.trimEnd();\n    if (opt.forceSingleLine) {\n      n.completionText = n.completionText.split(\"\\n\")[0];\n    }\n    return n;\n  }\n  \n  exports.ghostTextLogger = new M_logging_utils.Logger(\n    M_logging_utils.LogLevel.INFO,\n    \"ghostText\"\n  );\n  \n  // There seem to be 4 types of Results: Network, Cache, TypingAsSuggested, Cycling\n  (function (e) {\n    e[(e.Network = 0)] = \"Network\";\n    e[(e.Cache = 1)] = \"Cache\";\n    e[(e.TypingAsSuggested = 2)] = \"TypingAsSuggested\";\n    e[(e.Cycling = 3)] = \"Cycling\";\n  })((ResultType = exports.ResultType || (exports.ResultType = {})));\n  \n  // Cache size is 100 (key is prompt, val is a list of choices)\n  // Interestingly, the cache in the CopilotPanel workflow is of size 1e4.\n  exports.completionCache = new M_prompt_cache.LRUCache(100);\n  \n  const debouncer = new M_debouncer.Debouncer();\n  \n  // just updates the state variables\n  function R(e, t) {\n    I = e;\n    P = t;\n  }\n  \n  // options seems to be of form {multiline: bool, choices: list}\n  function cacheCompletion(ctx, promptWrapper, options) {\n    const promptCacheKey = M_prompt_cache.keyForPrompt(promptWrapper.prompt);\n    const cachedVal = exports.completionCache.get(promptCacheKey);\n    if (cachedVal && cachedVal.multiline === options.multiline) {\n      exports.completionCache.put(promptCacheKey, {\n        multiline: cachedVal.multiline,\n        choices: cachedVal.choices.concat(options.choices),\n      });\n    } else {\n      exports.completionCache.put(promptCacheKey, options);\n    }\n    exports.ghostTextLogger.debug(\n      ctx,\n      `Appended cached ghost text for key: ${promptCacheKey}, multiline: ${options.multiline}, number of suggestions: ${options.choices.length}`\n    );\n  }\n  \n  function getCachedChoices(promptCacheKey, n) {\n    const cachedVal = exports.completionCache.get(promptCacheKey);\n    if (cachedVal && (!n || cachedVal.multiline))\n        return cachedVal.choices;\n  }\n  \n  // not exactly sure what's going on here but seems like some adjustment for whitespaces\n  // i think it gets used when user keeps typing as suggestion is shown\n  function adjustCompletionForWS(choiceIndex, completionText, trailingWs) {\n    if (trailingWs.length > 0) {\n      if (completionText.startsWith(trailingWs))\n        return {\n          completionIndex: choiceIndex,\n          completionText: completionText,\n          displayText: completionText.substr(trailingWs.length),\n          displayNeedsWsOffset: !1,\n        };\n      {\n        const r = completionText.substr(0, completionText.length - completionText.trimLeft().length);\n        return trailingWs.startsWith(r)\n          ? {\n              completionIndex: choiceIndex,\n              completionText: completionText,\n              displayText: completionText.trimLeft(),\n              displayNeedsWsOffset: !0,\n            }\n          : {\n              completionIndex: choiceIndex,\n              completionText: completionText,\n              displayText: completionText,\n              displayNeedsWsOffset: !1,\n            };\n      }\n    }\n    return {\n      completionIndex: choiceIndex,\n      completionText: completionText,\n      displayText: completionText,\n      displayNeedsWsOffset: !1,\n    };\n  }\n  \n  function getExtendedTelemetryObj(ctx, n) {\n    const requestId = n.requestId;\n    const o = {\n      choiceIndex: n.choiceIndex.toString(),\n    };\n    const i = {\n      numTokens: n.numTokens,\n      compCharLen: n.completionText.length,\n      numLines: n.completionText.split(\"\\n\").length,\n    };\n    if (n.meanLogProb) {\n      i.meanLogProb = n.meanLogProb;\n    }\n    if (n.meanAlternativeLogProb) {\n      i.meanAlternativeLogProb = n.meanAlternativeLogProb;\n    }\n    const telemetryObj = n.telemetryData.extendedBy(o, i);\n    telemetryObj.extendWithRequestId(requestId);\n    telemetryObj.measurements.confidence = M_ghost_text_score_maybe.ghostTextScoreConfidence(\n      ctx,\n      telemetryObj\n    );\n    telemetryObj.measurements.quantile = M_ghost_text_score_maybe.ghostTextScoreQuantile(\n      ctx,\n      telemetryObj\n    );\n    exports.ghostTextLogger.debug(\n      ctx,\n      `Extended telemetry for ${n.telemetryData.properties.headerRequestId} with retention confidence ${telemetryObj.measurements.confidence} (expected as good or better than about ${telemetryObj.measurements.quantile} of all suggestions)`\n    );\n    return telemetryObj;\n  }\n  \n  function F(e, t, n, r, o) {\n    const i = Date.now() - r;\n    const s = i - o;\n    const a = n.telemetryData.extendedBy(\n      {},\n      {\n        completionCharLen: n.completionText.length,\n        requestTimeMs: i,\n        processingTimeMs: o,\n        deltaMs: s,\n        meanLogProb: n.meanLogProb || NaN,\n        meanAlternativeLogProb: n.meanAlternativeLogProb || NaN,\n        numTokens: n.numTokens,\n      }\n    );\n    a.extendWithRequestId(n.requestId);\n    M_telemetry_stuff.telemetry(e, `ghostText.${t}`, a);\n  }\n  \n  // The main stufffffffffff\n  exports.getGhostText = async function (ctx, doc, pos, isCycling, d, cancellationToken) {\n    var v;\n    var j;\n  \n    const promptWrapper = await M_prompt_extractor.extractPrompt(ctx, doc, pos);\n    if (\"contextTooShort\" === promptWrapper.type) {\n      exports.ghostTextLogger.debug(ctx, \"Breaking, not enough context\");\n      return {\n        type: \"abortedBeforeIssued\",\n        reason: \"Not enough context\",\n      };\n    }\n  \n    if (null == cancellationToken ? undefined : cancellationToken.isCancellationRequested) {\n      exports.ghostTextLogger.info(ctx, \"Cancelled after extractPrompt\");\n      return {\n        type: \"abortedBeforeIssued\",\n        reason: \"Cancelled after extractPrompt\",\n      };\n    }\n    \n    // 3 possible values:\n    // undefined -- cursor in middle of line, can't insert\n    // false -- cursor at end of line (maybe whitespace is there at the end but that's ignored)\n    // true -- some brackets/semicolons follow the cursor, but nothing significant.\n    const isMiddleOfLine = (function (doc, pos) {\n      const hasSuffixOnLine =\n        0 != doc.lineAt(pos).text.substr(pos.character).trim().length;\n  \n      const isSuffixIgnorable = (function (pos, doc) {\n        const suffixOnLine = doc.lineAt(pos).text.substr(pos.character).trim();\n        return /^\\s*[)}\\]\"'`]*\\s*[:{;,]?\\s*$/.test(suffixOnLine);\n      })(pos, doc);\n  \n      if (!hasSuffixOnLine || isSuffixIgnorable)\n          return hasSuffixOnLine && isSuffixIgnorable;\n    })(doc, pos);\n  \n    if (undefined === isMiddleOfLine) {\n      exports.ghostTextLogger.debug(ctx, \"Breaking, invalid middle of the line\");\n      return {\n        type: \"abortedBeforeIssued\",\n        reason: \"Invalid middle of the line\",\n      };\n    }\n  \n    const statusReporter = ctx.get(M_status_reporter_maybe.StatusReporter);\n    const locationFactory = ctx.get(M_location_factory.LocationFactory);\n    const requestOptions = await (async function (ctx, doc, pos, promptWrapper, isCycling, isMiddleOfLine) {\n      // not really sure what blockmode is...but seems language dependent.\n      const blockMode = await ctx\n        .get(M_config_stuff.BlockModeConfig)\n        .forLanguage(ctx, doc.languageId);\n  \n      switch (blockMode) {\n        case M_config_stuff.BlockMode.Server:\n          return {\n            blockMode: M_config_stuff.BlockMode.Server,\n            requestMultiline: true,\n            isCyclingRequest: isCycling,\n            finishedCb: async (e) => {},\n          };\n  \n        case M_config_stuff.BlockMode.Parsing:\n        case M_config_stuff.BlockMode.ParsingAndServer:\n        default: {\n          const requestMultiLine = await (async function (ctx, doc, pos, isMiddleOfLine) {\n            // huh, interesting. skip multiline if it's a long file?\n            if (doc.lineCount >= 8e3) {\n              M_telemetry_stuff.telemetry(\n                ctx,\n                \"ghostText.longFileMultilineSkip\",\n                M_telemetry_stuff.TelemetryData.createAndMarkAsIssued({\n                  languageId: doc.languageId,\n                  lineCount: String(doc.lineCount),\n                  currentLine: String(pos.line),\n                })\n              );\n            }\n            else {\n              if (\n                !isMiddleOfLine &&\n                M_getPrompt_main_stuff.isSupportedLanguageId(doc.languageId)\n              )\n                return await M_context_extractor_from_identation_maybe.isEmptyBlockStart(\n                  doc,\n                  pos\n                );\n              if (isMiddleOfLine && M_getPrompt_main_stuff.isSupportedLanguageId(doc.languageId))\n                return (\n                  (await M_context_extractor_from_identation_maybe.isEmptyBlockStart(\n                    doc,\n                    pos\n                  )) ||\n                  (await M_context_extractor_from_identation_maybe.isEmptyBlockStart(\n                    doc,\n                    doc.lineAt(pos).range.end\n                  ))\n                );\n            }\n            return false;\n          })(ctx, doc, pos, isMiddleOfLine);\n          return requestMultiLine\n            ? {\n                blockMode: blockMode,\n                requestMultiline: !0,\n                isCyclingRequest: !1,\n                finishedCb: async (r) => {\n                  // dk what `r` is. tried following calls to isBlockBodyFinished\n                  // but it's rather deeply\n                  let i;\n                  i =\n                    promptWrapper.trailingWs.length > 0 &&\n                    !promptWrapper.prompt.prefix.endsWith(promptWrapper.trailingWs)\n                      ? ctx\n                          .get(M_location_factory.LocationFactory)\n                          .position(\n                            pos.line,\n                            Math.max(pos.character - promptWrapper.trailingWs.length, 0)\n                          )\n                      : pos;\n                  return M_context_extractor_from_identation_maybe.isBlockBodyFinished(\n                    ctx,\n                    doc,\n                    i,\n                    r\n                  );\n                },\n              }\n            : {\n                blockMode: blockMode,\n                requestMultiline: !1,\n                isCyclingRequest: isCycling,\n                finishedCb: async (e) => {},\n              };\n        }\n      }\n    })(ctx, doc, pos, promptWrapper, isCycling, isMiddleOfLine);\n    if (null == cancellationToken ? undefined : cancellationToken.isCancellationRequested) {\n      exports.ghostTextLogger.info(ctx, \"Cancelled after requestMultiline\");\n      return {\n        type: \"abortedBeforeIssued\",\n        reason: \"Cancelled after requestMultiline\",\n      };\n    }\n  \n    // this looks like all text in doc till the cursor - but that's insane, especially\n    // if you consider the following code. Am I getting something wrong?\n    const [docTillCursor] = M_prompt_extractor.trimLastLine(\n      doc.getText(locationFactory.range(locationFactory.position(0, 0), pos))\n    );\n  \n    // get choices that either match whatever the user's typing\n    // otherwise return cached choices - I don't fully understand the details\n    // but this is what's being computed roughly\n    let choices = (function (ctx, docTillCursor, prompt, requestMultiline) {\n      // step1: filter out all cached choices that don't match whatever the user has typed\n      const cachedChoicesMatchingTypedText = (function (ctx, docTillCursor, requestMultiline) {\n        // I stores the doc till cursor from last \"snapshot\"\n        if (!I || !P || !docTillCursor.startsWith(I)) return;\n        const cachedChoices = getCachedChoices(P, requestMultiline);\n        if (!cachedChoices) return;\n        const i = docTillCursor.substring(I.length);\n        exports.ghostTextLogger.debug(\n          ctx,\n          `Getting completions for user-typing flow - remaining prefix: ${i}`\n        );\n        const updatedChoices = [];\n        cachedChoices.forEach((choice) => {\n          const trimmedChoice = trimCompletion(choice, {\n            forceSingleLine: false,\n          });\n          if (trimmedChoice.completionText.startsWith(i)) {\n            trimmedChoice.completionText = trimmedChoice.completionText.substring(i.length);\n            updatedChoices.push(trimmedChoice);\n          }\n        });\n        return updatedChoices;\n      })(ctx, docTillCursor, requestMultiline);\n  \n      if (cachedChoicesMatchingTypedText && cachedChoicesMatchingTypedText.length > 0)\n          return [cachedChoicesMatchingTypedText, ResultType.TypingAsSuggested];\n      \n      const cachedChoices = (function (ctx, n, prompt, requestMultiline) {\n        const promptCacheKey = M_prompt_cache.keyForPrompt(prompt);\n        exports.ghostTextLogger.debug(\n          ctx,\n          `Trying to get completions from cache for key: ${promptCacheKey}`\n        );\n        const cachedChoices = getCachedChoices(promptCacheKey, requestMultiline);\n        if (cachedChoices) {\n          exports.ghostTextLogger.debug(\n            ctx,\n            `Got completions from cache for key: ${promptCacheKey}`\n          );\n          const trimmedChoices = [];\n          cachedChoices.forEach((e) => {\n            const t = trimCompletion(e, {\n              forceSingleLine: !requestMultiline,\n            });\n            trimmedChoices.push(t);\n          });\n          // dk why this is there -- all choices would be non-empty ones, right?\n          // i think i'm missing something in this code.\n          const i = trimmedChoices.filter((e) => e.completionText);\n          if (i.length > 0) {\n            R(n, promptCacheKey);\n          }\n          return i;\n        }\n      })(ctx, docTillCursor, prompt, requestMultiline);\n      return cachedChoices && cachedChoices.length > 0 ? [cachedChoices, ResultType.Cache] : undefined;\n    })(ctx, docTillCursor, promptWrapper.prompt, requestOptions.requestMultiline);\n  \n    /* Now it looks like we're making request to the model */\n  \n    const requestId = M_uuid_utils.v4();\n    const repoInfo = M_background_context_provider.extractRepoInfoInBackground(\n      ctx,\n      doc.fileName\n    );\n    const engineUrl = await M_openai_conn_utils.getEngineURL(\n      ctx,\n      M_background_context_provider.tryGetGitHubNWO(repoInfo),\n      doc.languageId,\n      M_background_context_provider.getDogFood(repoInfo),\n      await M_background_context_provider.getUserKind(ctx),\n      d\n    );\n  \n    // I'm guessing this controls how long to wait before making request to model?\n    const delayMs = await ctx\n      .get(M_task_maybe.Features)\n      .beforeRequestWaitMs(\n        M_background_context_provider.tryGetGitHubNWO(repoInfo) || \"\",\n        doc.languageId\n      );\n  \n    // no clue what this parameter is for\n    const multiLogitBias = await ctx\n      .get(M_task_maybe.Features)\n      .multiLogitBias(\n        M_background_context_provider.tryGetGitHubNWO(repoInfo) || \"\",\n        doc.languageId\n      );\n  \n    const requestPayload = {\n      blockMode: requestOptions.blockMode,\n      languageId: doc.languageId,\n      repoInfo: repoInfo,\n      engineURL: engineUrl,\n      ourRequestId: requestId,\n      prefix: docTillCursor,\n      prompt: promptWrapper.prompt,\n      multiline: requestOptions.requestMultiline,\n      indentation: M_context_extractor_from_identation_maybe.contextIndentation(\n        doc,\n        pos\n      ),\n      isCycling: isCycling,\n      delayMs: delayMs,\n      multiLogitBias: multiLogitBias,\n    };\n  \n    const debouncePredict = await ctx.get(M_task_maybe.Features).debouncePredict();\n    const contextualFilterEnabled = await ctx.get(M_task_maybe.Features).contextualFilterEnable();\n    const contextualFilterAccThresh = await ctx\n      .get(M_task_maybe.Features)\n      .contextualFilterAcceptThreshold();\n    let ne = false;\n    if (debouncePredict || contextualFilterEnabled) {\n      ne = true;\n    }\n    const telemetryObject = (function (ctx, doc, requestPayload, pos, promptWrapper, d, ne) {\n      const locFactory = ctx.get(M_location_factory.LocationFactory);\n      const lineAtPos = doc.lineAt(pos.line);\n      const linePrefix = doc.getText(locFactory.range(lineAtPos.range.start, pos));\n      const lineSuffix = doc.getText(locFactory.range(pos, lineAtPos.range.end));\n      const d = {\n        languageId: doc.languageId,\n        beforeCursorWhitespace: JSON.stringify(\"\" === linePrefix.trim()),\n        afterCursorWhitespace: JSON.stringify(\"\" === lineSuffix.trim()),\n      };\n      const p = {\n        ...M_telemetry_stuff.telemetrizePromptLength(promptWrapper.prompt),\n        promptEndPos: doc.offsetAt(pos),\n        documentLength: doc.getText().length,\n        delayMs: requestPayload.delayMs,\n      };\n      const f = d.extendedBy(d, p);\n      f.properties.promptChoices = JSON.stringify(promptWrapper.promptChoices, (e, t) =>\n        t instanceof Map\n          ? Array.from(t.entries()).reduce(\n              (e, [t, n]) => ({\n                ...e,\n                [t]: n,\n              }),\n              {}\n            )\n          : t\n      );\n      f.properties.promptBackground = JSON.stringify(promptWrapper.promptBackground, (e, t) =>\n        t instanceof Map ? Array.from(t.values()) : t\n      );\n      f.measurements.promptComputeTimeMs = promptWrapper.computeTimeMs;\n      if (ne) {\n        f.measurements.contextualFilterScore =\n          M_contextual_filter_manager.contextualFilterScore(ctx, f, promptWrapper.prompt);\n      }\n      const m = requestPayload.repoInfo;\n      f.properties.gitRepoInformation =\n        undefined === m\n          ? \"unavailable\"\n          : m === M_background_context_provider.ComputationStatus.PENDING\n          ? \"pending\"\n          : \"available\";\n      if (\n        undefined !== m &&\n        m !== M_background_context_provider.ComputationStatus.PENDING\n      ) {\n        f.properties.gitRepoUrl = m.url;\n        f.properties.gitRepoHost = m.hostname;\n        f.properties.gitRepoOwner = m.owner;\n        f.properties.gitRepoName = m.repo;\n        f.properties.gitRepoPath = m.pathname;\n      }\n      f.properties.engineName = M_live_openai_fetcher.extractEngineName(\n        ctx,\n        requestPayload.engineURL\n      );\n      f.properties.isMultiline = JSON.stringify(requestPayload.multiline);\n      f.properties.blockMode = requestPayload.blockMode;\n      f.properties.isCycling = JSON.stringify(requestPayload.isCycling);\n      f.properties.headerRequestId = requestPayload.ourRequestId;\n      M_telemetry_stuff.telemetry(ctx, \"ghostText.issued\", f);\n      return f;\n    })(ctx, doc, requestPayload, pos, promptWrapper, d, ne);\n    \n    if (\n      (requestOptions.isCyclingRequest &&\n        (null !== (v = null == choices ? undefined : choices[0].length) && undefined !== v\n          ? v\n          : 0) > 1) ||\n      (!requestOptions.isCyclingRequest && undefined !== choices)\n    )\n      exports.ghostTextLogger.info(ctx, \"Found inline suggestions locally\");\n    else {\n      if (null == statusReporter) {\n        statusReporter.setProgress();\n      }\n      if (requestOptions.isCyclingRequest) {\n        const n = await (async function (ctx, requestPayload, telemetryObject, cancellationToken, finishedCb) {\n          return A(ctx, requestPayload, telemetryObject, cancellationToken, finishedCb, \"all completions\", async (i, s, a, c) => {\n            const l = [];\n            for await (const n of c) {\n              if (null == cancellationToken ? void 0 : cancellationToken.isCancellationRequested)\n                return (\n                  exports.ghostTextLogger.debug(\n                    ctx,\n                    \"Cancelled after awaiting choices iterator\"\n                  ),\n                  {\n                    type: \"canceled\",\n                    reason: \"after awaiting choices iterator\",\n                    telemetryData: (0,\n                    M_ghost_text_telemetry.mkCanceledResultTelemetry)(telemetryObject),\n                  }\n                );\n              if (n.completionText.trimEnd()) {\n                if (\n                  -1 !==\n                  l.findIndex(\n                    (e) => e.completionText.trim() === n.completionText.trim()\n                  )\n                )\n                  continue;\n                l.push(n);\n              }\n            }\n            return (\n              l.length > 0 &&\n                (cacheCompletion(ctx, requestPayload, {\n                  multiline: requestPayload.multiline,\n                  choices: l,\n                }),\n                F(ctx, \"cyclingPerformance\", l[0], s, a)),\n              {\n                type: \"success\",\n                value: l,\n                telemetryData: (0, M_ghost_text_telemetry.mkBasicResultTelemetry)(\n                  telemetryObject\n                ),\n                telemetryBlob: telemetryObject,\n              }\n            );\n          });\n        })(ctx, requestPayload, telemetryObject, cancellationToken, requestOptions.finishedCb);\n        if (\"success\" === n.type) {\n          const e =\n            null !== (j = null == choices ? void 0 : choices[0]) && void 0 !== j ? j : [];\n          n.value.forEach((t) => {\n            -1 ===\n              e.findIndex(\n                (e) => e.completionText.trim() === t.completionText.trim()\n              ) && e.push(t);\n          }),\n            (choices = [e, ResultType.Cycling]);\n        } else if (void 0 === choices) return null == statusReporter || statusReporter.removeProgress(), n;\n      } else {\n        const n = await (0, M_ghost_text_debouncer_maybe.getDebounceLimit)(ctx, telemetryObject);\n        try {\n          await debouncer.debounce(n);\n        } catch {\n          return {\n            type: \"canceled\",\n            reason: \"by debouncer\",\n            telemetryData: (0, M_ghost_text_telemetry.mkCanceledResultTelemetry)(\n              telemetryObject\n            ),\n          };\n        }\n        if (null == cancellationToken ? void 0 : cancellationToken.isCancellationRequested)\n          return (\n            exports.ghostTextLogger.info(ctx, \"Cancelled during debounce\"),\n            {\n              type: \"canceled\",\n              reason: \"during debounce\",\n              telemetryData: (0,\n              M_ghost_text_telemetry.mkCanceledResultTelemetry)(telemetryObject),\n            }\n          );\n        if (\n          contextualFilterEnabled &&\n          telemetryObject.measurements.contextualFilterScore &&\n          telemetryObject.measurements.contextualFilterScore < contextualFilterAccThresh / 100\n        )\n          return (\n            exports.ghostTextLogger.info(ctx, \"Cancelled by contextual filter\"),\n            {\n              type: \"canceled\",\n              reason: \"contextualFilterScore below threshold\",\n              telemetryData: (0,\n              M_ghost_text_telemetry.mkCanceledResultTelemetry)(telemetryObject),\n            }\n          );\n        const r = await (async function (e, n, r, o, s) {\n          return A(e, n, r, o, s, \"completions\", async (s, a, c, l) => {\n            const u = l[Symbol.asyncIterator](),\n              d = await u.next();\n            if (d.done)\n              return (\n                exports.ghostTextLogger.debug(e, \"All choices redacted\"),\n                {\n                  type: \"empty\",\n                  reason: \"all choices redacted\",\n                  telemetryData: (0,\n                  M_ghost_text_telemetry.mkBasicResultTelemetry)(r),\n                }\n              );\n            if (null == o ? void 0 : o.isCancellationRequested)\n              return (\n                exports.ghostTextLogger.debug(\n                  e,\n                  \"Cancelled after awaiting redactedChoices iterator\"\n                ),\n                {\n                  type: \"canceled\",\n                  reason: \"after awaiting redactedChoices iterator\",\n                  telemetryData: (0,\n                  M_ghost_text_telemetry.mkCanceledResultTelemetry)(r),\n                }\n              );\n            const p = d.value;\n            if (void 0 === p)\n              return (\n                exports.ghostTextLogger.debug(\n                  e,\n                  \"Got undefined choice from redactedChoices iterator\"\n                ),\n                {\n                  type: \"empty\",\n                  reason: \"got undefined choice from redactedChoices iterator\",\n                  telemetryData: (0,\n                  M_ghost_text_telemetry.mkBasicResultTelemetry)(r),\n                }\n              );\n            F(e, \"performance\", p, a, c);\n            const h = s - 1;\n            exports.ghostTextLogger.debug(\n              e,\n              `Awaited first result, id:  ${p.choiceIndex}`\n            ),\n              (function (e, n, r) {\n                const o = (0, M_prompt_cache.keyForPrompt)(n.prompt);\n                R(n.prefix, o),\n                  exports.completionCache.put(o, r),\n                  exports.ghostTextLogger.debug(\n                    e,\n                    `Cached ghost text for key: ${o}, multiline: ${r.multiline}, number of suggestions: ${r.choices.length}`\n                  );\n              })(e, n, {\n                multiline: n.multiline,\n                choices: [p],\n              });\n            const f = [];\n            for (let e = 0; e < h; e++) f.push(u.next());\n            const m = Promise.all(f).then((r) => {\n              exports.ghostTextLogger.debug(\n                e,\n                `Awaited remaining results, number of results: ${r.length}`\n              );\n              const o = [];\n              for (const n of r) {\n                const r = n.value;\n                if (\n                  void 0 !== r &&\n                  (exports.ghostTextLogger.info(\n                    e,\n                    `GhostText later completion: [${r.completionText}]`\n                  ),\n                  r.completionText.trimEnd())\n                ) {\n                  if (\n                    -1 !==\n                    o.findIndex(\n                      (e) => e.completionText.trim() === r.completionText.trim()\n                    )\n                  )\n                    continue;\n                  if (r.completionText.trim() === p.completionText.trim())\n                    continue;\n                  o.push(r);\n                }\n              }\n              o.length > 0 &&\n                cacheCompletion(e, n, {\n                  multiline: n.multiline,\n                  choices: o,\n                });\n            });\n            return (\n              (0, M_runtime_mode_maybe.isRunningInTest)(e) && (await m),\n              {\n                type: \"success\",\n                value: trimCompletion(d.value, {\n                  forceSingleLine: !1,\n                }),\n                telemetryData: (0, M_ghost_text_telemetry.mkBasicResultTelemetry)(\n                  r\n                ),\n                telemetryBlob: r,\n              }\n            );\n          });\n        })(ctx, requestPayload, telemetryObject, cancellationToken, requestOptions.finishedCb);\n        if (\"success\" !== r.type) return null == statusReporter || statusReporter.removeProgress(), r;\n        choices = [[r.value], ResultType.Network];\n      }\n      if (null == statusReporter) {\n        statusReporter.removeProgress();\n      }\n    }\n    if (undefined === choices)\n      return {\n        type: \"failed\",\n        reason: \"internal error: choices should be defined after network call\",\n        telemetryData: M_ghost_text_telemetry.mkBasicResultTelemetry(telemetryObject),\n      };\n    const [oe, ie] = choices;\n    const se = M_async_iterable_utils_maybe.asyncIterableMapFilter(\n      M_async_iterable_utils_maybe.asyncIterableFromArray(oe),\n      async (r) =>\n        M_postprocess_choice.postProcessChoice(\n          ctx,\n          \"ghostText\",\n          doc,\n          pos,\n          r,\n          isMiddleOfLine,\n          exports.ghostTextLogger\n        )\n    );\n    const ae = [];\n    for await (const r of se) {\n      const o = isMiddleOfLine && M_postprocess_choice.checkSuffix(doc, pos, r);\n      if (null == cancellationToken ? undefined : cancellationToken.isCancellationRequested) {\n        exports.ghostTextLogger.info(\n          ctx,\n          \"Cancelled after post processing completions\"\n        );\n        return {\n          type: \"canceled\",\n          reason: \"after post processing completions\",\n          telemetryData: M_ghost_text_telemetry.mkCanceledResultTelemetry(telemetryObject),\n        };\n      }\n      const i = getExtendedTelemetryObj(ctx, r);\n      const a = {\n        completion: adjustCompletionForWS(r.choiceIndex, r.completionText, promptWrapper.trailingWs),\n        telemetry: i,\n        isMiddleOfTheLine: isMiddleOfLine,\n        coversSuffix: o,\n      };\n      ae.push(a);\n    }\n    return {\n      type: \"success\",\n      value: [ae, ie],\n      telemetryData: M_ghost_text_telemetry.mkBasicResultTelemetry(telemetryObject),\n      telemetryBlob: telemetryObject,\n    };\n  };\n  ",
    "3055125": "// neighbor-snippet-selector.js\nObject.defineProperty(exports, \"__esModule\", {\n  value: !0,\n});\nexports.getNeighborSnippets = exports.neighborOptionToSelection = undefined;\n\nconst M_language_marker_constants = require(\"language-marker-constants\");\nconst M_jaccard_scorer = require(\"jaccard-scorer\");\n\nfunction compareThisSnippet(e) {\n  return [\n    e.relativePath\n      ? \"Compare this snippet from \" + e.relativePath + \":\"\n      : \"Compare this snippet:\",\n  ].concat(e.snippet.split(\"\\n\"));\n}\n\nexports.neighborOptionToSelection = {\n  none: {\n    matcherFactory: M_jaccard_scorer.FixedWindowSizeJaccardMatcher.FACTORY(1),\n    threshold: -1,\n    numberOfSnippets: 0,\n  },\n  conservative: {\n    matcherFactory: M_jaccard_scorer.FixedWindowSizeJaccardMatcher.FACTORY(10),\n    threshold: 0.3,\n    numberOfSnippets: 1,\n  },\n  medium: {\n    matcherFactory: M_jaccard_scorer.FixedWindowSizeJaccardMatcher.FACTORY(20),\n    threshold: 0.1,\n    numberOfSnippets: 2,\n  },\n  eager: {\n    matcherFactory: M_jaccard_scorer.FixedWindowSizeJaccardMatcher.FACTORY(60),\n    threshold: 0,\n    numberOfSnippets: 4,\n  },\n  eagerButLittle: {\n    matcherFactory: M_jaccard_scorer.FixedWindowSizeJaccardMatcher.FACTORY(10),\n    threshold: 0,\n    numberOfSnippets: 1,\n  },\n};\n\nexports.getNeighborSnippets = async function (\n  curFile,\n  relevantDocs,\n  neighborTabsOpt,\n  indentationMinLengthOpt,\n  indentationMaxLengthOpt,\n  snippetSelectionOpt,\n  snippetSelectionK\n) {\n  const nbrOpt = exports.neighborOptionToSelection[neighborTabsOpt];\n\n  // nbrMatchers is an instance of JaccardMatcher (set to compare to curFile)\n  const nbrMatcher = (function (\n    curFile,\n    neighborTabsOpt,\n    indentationMinLengthOpt,\n    indentationMaxLengthOpt\n  ) {\n    const nbrOpt = {\n      ...exports.neighborOptionToSelection[neighborTabsOpt],\n    };\n    if (\n      undefined !== indentationMinLengthOpt &&\n      undefined !== indentationMaxLengthOpt\n    ) {\n      nbrOpt.matcherFactory =\n        M_jaccard_scorer.IndentationBasedJaccardMatcher.FACTORY(\n          indentationMinLengthOpt,\n          indentationMaxLengthOpt\n        );\n    }\n    return nbrOpt.matcherFactory.to(curFile);\n  })(\n    curFile,\n    neighborTabsOpt,\n    indentationMinLengthOpt,\n    indentationMaxLengthOpt\n  );\n\n  // go through each relevantDoc and find matches\n  return (\n    relevantDocs\n      // ignore files that are too long or empty\n      .filter((e) => e.source.length < 1e4 && e.source.length > 0)\n      // select the first 20 files (already sorted by access time)\n      .slice(0, 20)\n      // search for matches in each file\n      .reduce(\n        (allMatches, curRelFile) =>\n          allMatches.concat(\n            // for curRelFile, find matches, and add relativePath to each match\n            nbrMatcher\n              .findMatches(curRelFile, snippetSelectionOpt, snippetSelectionK)\n              .map((e) => ({\n                relativePath: curRelFile.relativePath,\n                ...e,\n              }))\n          ),\n        []\n      )\n      // so far, the pipeline has \"allMatches\", which is an array of matches from all files\n      // now filter out matches with low scores\n      .filter((e) => e.score && e.snippet && e.score > nbrOpt.threshold)\n      // sort by score\n      .sort((e, t) => e.score - t.score)\n      // take the last n matches (the ones with the highest scores)\n      .slice(-nbrOpt.numberOfSnippets)\n      // format the matches\n      .map((t) => ({\n        score: t.score,\n        snippet: compareThisSnippet(t)\n          .map(\n            (t) =>\n              M_language_marker_constants.comment(t, curFile.languageId) + \"\\n\"\n          )\n          .join(\"\"),\n        startLine: t.startLine,\n        endLine: t.endLine,\n      }))\n  );\n};\n",
    "3055179": "// imports-and-docs-extractor.js\nObject.defineProperty(exports, \"__esModule\", {\n    value: !0,\n});\nexports.extractLocalImportContext = exports.getDocComment = undefined;\n\nconst M_path = require(\"path\");\nconst M_get_prompt_parsing_utils = require(\"get-prompt-parsing-utils\");\n\nfunction i(e, t) {\n    var n;\n    let o =\n        null === (n = t.namedChild(1)) || undefined === n\n            ? undefined\n            : n.text.slice(1, -1);\n    if (!o || !o.startsWith(\".\")) return null;\n    if (\"\" === M_path.extname(o)) o += \".ts\";\n    else if (\".ts\" !== M_path.extname(o)) return null;\n    return M_path.join(M_path.dirname(e), o);\n}\n\nfunction s(e) {\n    var t;\n    var n;\n    var r;\n    var o;\n    var i;\n    let s = [];\n    if (\n        \"import_clause\" ===\n        (null === (t = e.namedChild(0)) || undefined === t ? undefined : t.type)\n    ) {\n        let t = e.namedChild(0);\n        if (\n            \"named_imports\" ===\n            (null === (n = null == t ? undefined : t.namedChild(0)) || undefined === n\n                ? undefined\n                : n.type)\n        ) {\n            let e = t.namedChild(0);\n            for (let t of null !== (r = null == e ? undefined : e.namedChildren) &&\n                undefined !== r\n                ? r\n                : [])\n                if (\"import_specifier\" === t.type) {\n                    const e =\n                        null === (o = t.childForFieldName(\"name\")) || undefined === o\n                            ? undefined\n                            : o.text;\n                    if (e) {\n                        const n =\n                            null === (i = t.childForFieldName(\"alias\")) || undefined === i\n                                ? undefined\n                                : i.text;\n                        s.push({\n                            name: e,\n                            alias: n,\n                        });\n                    }\n                }\n        }\n    }\n    return s;\n}\n\nconst a = new Map();\n\nfunction c(e, t) {\n    var n;\n    var r;\n    let o =\n        null !==\n            (r =\n                null === (n = null == t ? undefined : t.childForFieldName(\"name\")) ||\n                    undefined === n\n                    ? undefined\n                    : n.text) && undefined !== r\n            ? r\n            : \"\";\n    switch (null == t ? undefined : t.type) {\n        case \"ambient_declaration\":\n            return c(e, t.namedChild(0));\n        case \"interface_declaration\":\n        case \"enum_declaration\":\n        case \"type_alias_declaration\":\n            return {\n                name: o,\n                decl: t.text,\n            };\n        case \"function_declaration\":\n        case \"function_signature\":\n            return {\n                name: o,\n                decl: l(e, t),\n            };\n        case \"class_declaration\": {\n            let n = (function (e, t) {\n                let n = t.childForFieldName(\"body\");\n                if (n) return n.namedChildren.map((t) => d(e, t)).filter((e) => e);\n            })(e, t);\n            let r = \"\";\n            if (n) {\n                let o = t.childForFieldName(\"body\");\n                r = `declare ${e.substring(t.startIndex, o.startIndex + 1)}`;\n                r += n.map((e) => \"\\n\" + e).join(\"\");\n                r += \"\\n}\";\n            }\n            return {\n                name: o,\n                decl: r,\n            };\n        }\n    }\n    return {\n        name: o,\n        decl: \"\",\n    };\n}\n\nfunction l(e, t) {\n    var n;\n    var r;\n    var o;\n    const i =\n        null !==\n            (r =\n                null === (n = t.childForFieldName(\"return_type\")) || undefined === n\n                    ? undefined\n                    : n.endIndex) && undefined !== r\n            ? r\n            : null === (o = t.childForFieldName(\"parameters\")) || undefined === o\n                ? undefined\n                : o.endIndex;\n    if (undefined !== i) {\n        let n = e.substring(t.startIndex, i) + \";\";\n        return \"function_declaration\" === t.type || \"function_signature\" === t.type\n            ? \"declare \" + n\n            : n;\n    }\n    return \"\";\n}\n\nfunction getDocComment(e, t) {\n    const n = M_get_prompt_parsing_utils.getFirstPrecedingComment(t);\n    return n ? e.substring(n.startIndex, t.startIndex) : \"\";\n}\n\nfunction d(e, t) {\n    var n;\n    var r;\n    var i;\n    var s;\n    var a;\n    if (\n        \"accessibility_modifier\" ===\n        (null === (n = null == t ? undefined : t.firstChild) || undefined === n\n            ? undefined\n            : n.type) &&\n        \"private\" === t.firstChild.text\n    )\n        return \"\";\n    const c = M_get_prompt_parsing_utils.getFirstPrecedingComment(t);\n    const p =\n        null !==\n            (r = (function (e, t) {\n                let n = t.startIndex - 1;\n                for (; n >= 0 && (\" \" === e[n] || \"\\t\" === e[n]);) n--;\n                if (n < 0 || \"\\n\" === e[n]) return e.substring(n + 1, t.startIndex);\n            })(e, null != c ? c : t)) && undefined !== r\n            ? r\n            : \"  \";\n    const h = getDocComment(e, t);\n    switch (t.type) {\n        case \"ambient_declaration\":\n            const n = t.namedChild(0);\n            return n ? p + h + d(e, n) : \"\";\n        case \"method_definition\":\n        case \"method_signature\":\n            return p + h + l(e, t);\n        case \"public_field_definition\": {\n            let n =\n                null !==\n                    (s =\n                        null === (i = t.childForFieldName(\"type\")) || undefined === i\n                            ? undefined\n                            : i.endIndex) && undefined !== s\n                    ? s\n                    : null === (a = t.childForFieldName(\"name\")) || undefined === a\n                        ? undefined\n                        : a.endIndex;\n            if (undefined !== n) return p + h + e.substring(t.startIndex, n) + \";\";\n        }\n    }\n    return \"\";\n}\n\nasync function p(e, t, n) {\n    let r = new Map();\n    let i = -1;\n    try {\n        i = await n.mtime(e);\n    } catch {\n        return r;\n    }\n    let s = a.get(e);\n    if (s && s.mtime === i) return s.exports;\n    if (\"typescript\" === t) {\n        let i = null;\n        try {\n            let s = (await n.readFile(e)).toString();\n            i = await M_get_prompt_parsing_utils.parseTree(t, s);\n            for (let e of M_get_prompt_parsing_utils.queryExports(t, i.rootNode))\n                for (let t of e.captures) {\n                    let e = t.node;\n                    if (\"export_statement\" === e.type) {\n                        let t = e.childForFieldName(\"declaration\");\n                        if (null == t ? undefined : t.hasError()) continue;\n                        let { name: n, decl: o } = c(s, t);\n                        if (n) {\n                            o = getDocComment(s, e) + o;\n                            let t = r.get(n);\n                            if (t) {\n                                t = [];\n                                r.set(n, t);\n                            }\n                            t.push(o);\n                        }\n                    }\n                }\n        } catch {\n        } finally {\n            if (i) {\n                i.delete();\n            }\n        }\n    }\n    if (a.size > 2e3)\n        for (let e of a.keys()) {\n            a.delete(e);\n            if (r.size <= 1e3) break;\n        }\n    a.set(e, {\n        mtime: i,\n        exports: r,\n    });\n    return r;\n}\n\nexports.getDocComment = getDocComment;\n\nconst h = /^\\s*import\\s*(type|)\\s*\\{[^}]*\\}\\s*from\\s*['\"]\\./gm;\n\nexports.extractLocalImportContext = async function (e, t) {\n    let { source: n, uri: r, languageId: a } = e;\n    return t && \"typescript\" === a\n        ? (async function (e, t, n) {\n            let r = \"typescript\";\n            let a = [];\n            const c = (function (e) {\n                let t;\n                let n = -1;\n                h.lastIndex = -1;\n                do {\n                    t = h.exec(e);\n                    if (t) {\n                        n = h.lastIndex + t.length;\n                    }\n                } while (t);\n                if (-1 === n) return -1;\n                const r = e.indexOf(\"\\n\", n);\n                return -1 !== r ? r : e.length;\n            })(e);\n            if (-1 === c) return a;\n            e = e.substring(0, c);\n            let l = await M_get_prompt_parsing_utils.parseTree(r, e);\n            try {\n                for (let e of (function (e) {\n                    let t = [];\n                    for (let n of e.namedChildren)\n                        if (\"import_statement\" === n.type) {\n                            t.push(n);\n                        }\n                    return t;\n                })(l.rootNode)) {\n                    let o = i(t, e);\n                    if (!o) continue;\n                    let c = s(e);\n                    if (0 === c.length) continue;\n                    let l = await p(o, r, n);\n                    for (let e of c)\n                        if (l.has(e.name)) {\n                            a.push(...l.get(e.name));\n                        }\n                }\n            } finally {\n                l.delete();\n            }\n            return a;\n        })(n, r, t)\n        : [];\n};\n",
    "3055312": "// get-prompt-actual.js\nObject.defineProperty(exports, \"__esModule\", {\n    value: true,\n});\nexports.getPrompt =\n    exports.newLineEnded =\n    exports.normalizeLanguageId =\n    exports.PromptOptions =\n    exports.SuffixStartMode =\n    exports.SuffixMatchOption =\n    exports.SuffixOption =\n    exports.LineEndingOptions =\n    exports.LocalImportContextOption =\n    exports.SnippetSelectionOption =\n    exports.NeighboringTabsPositionOption =\n    exports.NeighboringTabsOption =\n    exports.SiblingOption =\n    exports.PathMarkerOption =\n    exports.LanguageMarkerOption =\n    exports.TOKENS_RESERVED_FOR_SUFFIX_ENCODING =\n    exports.MAX_EDIT_DISTANCE_LENGTH =\n    exports.MAX_PROMPT_LENGTH =\n    undefined;\nconst M_language_marker_constants = require(\"language-marker-constants\");\nconst M_imports_and_docs_extractor = require(\"imports-and-docs-extractor\");\nconst M_neighbor_snippet_selector = require(\"neighbor-snippet-selector\");\nconst M_sibling_function_fetcher = require(\"sibling-function-fetcher\");\nconst M_tokenizer = require(\"tokenizer\");\nconst M_prompt_choices_and_wishlist = require(\"prompt-choices-and-wishlist\");\nconst M_edit_distance = require(\"edit-distance\");\n\n// this thing's VERY weird.\n// Looks like the getPrompt function remembers the previous used\n// suffix, and if the suffix extracted currently is \"roughly\" the same\n// then it uses the previous suffix.\n// This is some weird caching thing, which idk why it's here.\n// Probably the backend model can benefit from cached suffixes?????\n// How? I can't imagine how.\n// \"roughly the same\" computation is defined by the SuffixMatchOption (Equal/Levenshtein)\nlet cachedSuffix = {\n    text: \"\",\n    tokens: [],\n};\n\nvar LanguageMarkerOption; // NoMarker, Top, Always\nvar PathMarkerOption; // NoMarker, Top, Always\nvar SiblingOption; // NoSiblings, SiblingsOverContext, ContextOverSiblings\nvar NeighboringTabsOption; // None, Conservative, Medium, Eager, EagerButLittle\nvar NeighboringTabsPositionOption; // TopOfText, DirectlyAboveCursor, AfterSiblings\nvar SnippetSelectionOption; // BestMatch, TopK\nvar LocalImportContextOption; // NoContext, Declarations\nvar LineEndingOptions; // ConvertToUnix, KeepOriginal\nvar SuffixMatchOption; // Equal, Levenshtein\nvar SuffixStartMode; // Cursor, CursorTrimStart, SiblingBlock, SiblingBlockTrimStart\nvar SuffixOption; // None, FifteenPercent\n\n// Prompt can only have 1500 tokens\nexports.MAX_PROMPT_LENGTH = 1500;\n// This variable controls how cached suffix vs current suffix are compared\n// if SuffixMatchOption is Levenshtein, then the edit distance is computed\n// on the first 50 tokens of the suffix and the cached suffix.\n// Again, I find this really weird.\nexports.MAX_EDIT_DISTANCE_LENGTH = 50;\n// Not sure how this encoding works. Maybe at a parent level when it\n// is all converted to a string. Dk if that happens in the extension\n// or a backend.\nexports.TOKENS_RESERVED_FOR_SUFFIX_ENCODING = 5;\n\n(function (e) {\n    e.NoMarker = \"nomarker\";\n    e.Top = \"top\";\n    e.Always = \"always\";\n})((LanguageMarkerOption = exports.LanguageMarkerOption || (exports.LanguageMarkerOption = {})));\n\n(function (e) {\n    e.NoMarker = \"nomarker\";\n    e.Top = \"top\";\n    e.Always = \"always\";\n})((PathMarkerOption = exports.PathMarkerOption || (exports.PathMarkerOption = {})));\n\n(function (e) {\n    e.NoSiblings = \"nosiblings\";\n    e.SiblingsOverContext = \"siblingabove\";\n    e.ContextOverSiblings = \"contextabove\";\n})((SiblingOption = exports.SiblingOption || (exports.SiblingOption = {})));\n\n(function (e) {\n    e.None = \"none\";\n    e.Conservative = \"conservative\";\n    e.Medium = \"medium\";\n    e.Eager = \"eager\";\n    e.EagerButLittle = \"eagerButLittle\";\n})((NeighboringTabsOption = exports.NeighboringTabsOption || (exports.NeighboringTabsOption = {})));\n\n(function (e) {\n    e.TopOfText = \"top\";\n    e.DirectlyAboveCursor = \"aboveCursor\";\n    e.AfterSiblings = \"afterSiblings\";\n})(\n    (NeighboringTabsPositionOption =\n        exports.NeighboringTabsPositionOption ||\n        (exports.NeighboringTabsPositionOption = {}))\n);\n\n(function (e) {\n    e.BestMatch = \"bestMatch\";\n    e.TopK = \"topK\";\n})(\n    (SnippetSelectionOption = exports.SnippetSelectionOption || (exports.SnippetSelectionOption = {}))\n);\n\n(function (e) {\n    e.NoContext = \"nocontext\";\n    e.Declarations = \"declarations\";\n})(\n    (LocalImportContextOption =\n        exports.LocalImportContextOption || (exports.LocalImportContextOption = {}))\n);\n\n(function (e) {\n    e.ConvertToUnix = \"unix\";\n    e.KeepOriginal = \"keep\";\n})((LineEndingOptions = exports.LineEndingOptions || (exports.LineEndingOptions = {})));\n\n(SuffixOption = exports.SuffixOption || (exports.SuffixOption = {})).None = \"none\";\nSuffixOption.FifteenPercent = \"fifteenPercent\";\n\n(function (e) {\n    e.Equal = \"equal\";\n    e.Levenshtein = \"levenshteineditdistance\";\n})((SuffixMatchOption = exports.SuffixMatchOption || (exports.SuffixMatchOption = {})));\n\n(function (e) {\n    e.Cursor = \"cursor\";\n    e.CursorTrimStart = \"cursortrimstart\";\n    e.SiblingBlock = \"siblingblock\";\n    e.SiblingBlockTrimStart = \"siblingblocktrimstart\";\n})((SuffixStartMode = exports.SuffixStartMode || (exports.SuffixStartMode = {})));\n\nclass PromptOptions {\n    constructor(fs, kwargs) {\n        this.fs = fs;\n        this.maxPromptLength = exports.MAX_PROMPT_LENGTH;\n        this.languageMarker = LanguageMarkerOption.Top;\n        this.pathMarker = PathMarkerOption.Top;\n        this.includeSiblingFunctions = SiblingOption.ContextOverSiblings;\n        this.localImportContext = LocalImportContextOption.Declarations;\n        this.neighboringTabs = NeighboringTabsOption.Eager;\n        this.neighboringTabsPosition = NeighboringTabsPositionOption.TopOfText;\n        this.lineEnding = LineEndingOptions.ConvertToUnix;\n        this.suffixPercent = 0;\n        this.suffixStartMode = SuffixStartMode.Cursor;\n        this.suffixMatchThreshold = 0;\n        this.suffixMatchCriteria = SuffixMatchOption.Levenshtein;\n        this.fimSuffixLengthThreshold = 0;\n        // override defaults with kwargs\n        if (kwargs) for (const e in kwargs) this[e] = kwargs[e];\n        if (this.suffixPercent < 0 || this.suffixPercent > 100)\n            throw new Error(\n                `suffixPercent must be between 0 and 100, but was ${this.suffixPercent}`\n            );\n        if (this.suffixPercent > 0 && this.includeSiblingFunctions != SiblingOption.NoSiblings)\n            throw new Error(\n                `Invalid option combination. Cannot set suffixPercent > 0 (${this.suffixPercent}) and includeSiblingFunctions ${this.includeSiblingFunctions}`\n            );\n        if (this.suffixMatchThreshold < 0 || this.suffixMatchThreshold > 100)\n            throw new Error(\n                `suffixMatchThreshold must be at between 0 and 100, but was ${this.suffixMatchThreshold}`\n            );\n        if (this.fimSuffixLengthThreshold < -1)\n            throw new Error(\n                `fimSuffixLengthThreshold must be at least -1, but was ${this.fimSuffixLengthThreshold}`\n            );\n        if (\n            null != this.indentationMinLength &&\n            null != this.indentationMaxLength &&\n            this.indentationMinLength > this.indentationMaxLength\n        )\n            throw new Error(\n                `indentationMinLength must be less than or equal to indentationMaxLength, but was ${this.indentationMinLength} and ${this.indentationMaxLength}`\n            );\n        if (\n            this.snippetSelection === SnippetSelectionOption.TopK &&\n            undefined === this.snippetSelectionK\n        )\n            throw new Error(\"snippetSelectionK must be defined.\");\n        if (\n            this.snippetSelection === SnippetSelectionOption.TopK &&\n            this.snippetSelectionK &&\n            this.snippetSelectionK <= 0\n        )\n            throw new Error(\n                `snippetSelectionK must be greater than 0, but was ${this.snippetSelectionK}`\n            );\n    }\n}\nexports.PromptOptions = PromptOptions;\n\nconst E = {\n    javascriptreact: \"javascript\",\n    jsx: \"javascript\",\n    typescriptreact: \"typescript\",\n    jade: \"pug\",\n    cshtml: \"razor\",\n};\n\nfunction normalizeLanguageId(langId) {\n    var t;\n    langId = langId.toLowerCase();\n    return null !== (t = E[langId]) && undefined !== t ? t : langId;\n}\n\n// ensure that the string ends with a newline except for empty strings\nfunction newLineEnded(str) {\n    return \"\" == str || str.endsWith(\"\\n\") ? str : str + \"\\n\";\n}\n\nexports.normalizeLanguageId = normalizeLanguageId;\nexports.newLineEnded = newLineEnded;\n\n// type of relevantDocs[i] ==> {uri, source, relativePath, languageId}\n// type of curFile ==> relevantDocs[i] + {offset}\nexports.getPrompt = async function (fs, curFile, promptOpts = {}, relevantDocs = []) {\n    var suffixVsCachedSuffixEditDist;\n    const promptOptions = new PromptOptions(fs, promptOpts);\n    let useCachedSuffix = false;\n\n    const { source: curSrc, offset: offset } = curFile;\n    if (offset < 0 || offset > curSrc.length)\n        throw new Error(`Offset ${offset} is out of range.`);\n\n    curFile.languageId = normalizeLanguageId(curFile.languageId);\n    // Priorities is a class with helper methods to create prioritized properties\n    const priorities = new M_prompt_choices_and_wishlist.Priorities();\n    const beforeCursorPriority = priorities.justBelow(M_prompt_choices_and_wishlist.Priorities.TOP);\n    const languageMarkerPriority =\n        promptOptions.languageMarker == LanguageMarkerOption.Always\n            ? priorities.justBelow(M_prompt_choices_and_wishlist.Priorities.TOP)\n            : priorities.justBelow(beforeCursorPriority);\n    const pathMarkerPriority =\n        promptOptions.pathMarker == PathMarkerOption.Always\n            ? priorities.justBelow(M_prompt_choices_and_wishlist.Priorities.TOP)\n            : priorities.justBelow(beforeCursorPriority);\n    const siblingsPriority =\n        promptOptions.includeSiblingFunctions == SiblingOption.ContextOverSiblings\n            ? priorities.justBelow(beforeCursorPriority)\n            : priorities.justAbove(beforeCursorPriority);\n    const localImportPriority = priorities.justBelow(beforeCursorPriority, siblingsPriority);\n    const similarSnippetPriority = priorities.justBelow(localImportPriority);\n\n    // PromptWishlist appears to be a class to which:\n    // (a) you add a wishlist of elements you want in the final prompt\n    //      along with the priorities you have in mind for each element\n    // (b) and at the end it lets you \"fulfill\" the wishlist\n    //      based on some constraints (prompt size, priorities)\n    const pWishlist = new M_prompt_choices_and_wishlist.PromptWishlist(promptOptions.lineEnding);\n    let langMarkerElemIdxs; // indices of language markers in the wishlist\n    let pathMarkerElemIdxs; // indices of path markers in the wishlist\n\n    // if language-marker option is enabled, add current file's language marker\n    // to the wishlist\n    if (promptOptions.languageMarker != LanguageMarkerOption.NoMarker) {\n        // e.g., `#!/usr/bin/env python3` for python, or `Language: \n        const e = newLineEnded(M_language_marker_constants.getLanguageMarker(curFile));\n        langMarkerElemIdxs = pWishlist.append(\n            e,\n            M_prompt_choices_and_wishlist.PromptElementKind.LanguageMarker,\n            languageMarkerPriority\n        );\n    }\n\n    // if path-marker option is enabled, add current file's path marker to the wishlist\n    if (promptOptions.pathMarker != PathMarkerOption.NoMarker) {\n        const e = newLineEnded(M_language_marker_constants.getPathMarker(curFile));\n        if (e.length > 0) {\n            pathMarkerElemIdxs = pWishlist.append(\n                e,\n                M_prompt_choices_and_wishlist.PromptElementKind.PathMarker,\n                pathMarkerPriority\n            );\n        }\n    }\n\n    // if local-import-context option is enabled, add local import context to the wishlist\n    if (promptOptions.localImportContext != LocalImportContextOption.NoContext) {\n        // at least in current version, this seems to be defined only for typescript\n        // and basically that returns a list of imported symbols\n        for (const e of await M_imports_and_docs_extractor.extractLocalImportContext(\n            curFile,\n            promptOptions.fs\n        )) {\n            pWishlist.append(\n                newLineEnded(e),\n                M_prompt_choices_and_wishlist.PromptElementKind.ImportedFile,\n                localImportPriority\n            );\n        }\n    }\n    \n    // if neighboringTabs option is enabled and we have relevant docs, collect snippets from\n    // those files.\n    // This contains the \"Compare this snippet from `path/to/file`:<snip>\" parts of the prompt\n    const neighborSnippets =\n        promptOptions.neighboringTabs == NeighboringTabsOption.None || 0 == relevantDocs.length\n            ? []\n            : await M_neighbor_snippet_selector.getNeighborSnippets(\n                curFile,\n                relevantDocs,\n                promptOptions.neighboringTabs,\n                promptOptions.indentationMinLength,\n                promptOptions.indentationMaxLength,\n                promptOptions.snippetSelectionOption,\n                promptOptions.snippetSelectionK\n            );\n    function addSnippetsToWishlist() {\n        neighborSnippets.forEach((nbrSnip) =>\n            pWishlist.append(\n                nbrSnip.snippet,\n                M_prompt_choices_and_wishlist.PromptElementKind.SimilarFile,\n                similarSnippetPriority,\n                M_tokenizer.tokenLength(nbrSnip.snippet),\n                nbrSnip.score\n            )\n        );\n    }\n    // dk why this condition exists.\n    // also dk who sets this option.\n    // At least `prompt-extractor.js` (probably the only caller of this function)\n    // doesn't set this option. Which means the default value from PromptOptions\n    // is used, which is `TopOfText`.\n    //\n    // tldr: this condition seems to always be true.\n    if (promptOptions.neighboringTabsPosition == NeighboringTabsPositionOption.TopOfText) {\n        addSnippetsToWishlist();\n    }\n\n    // index in the wishlist of the elements that are of type `BeforeCursor`\n    const beforeCursorElemIdxs = [];\n    // I'm not sure what this is supposed to mean\n    let U;\n\n    // if sibling-functions option is disabled, `U` is the text before the cursor\n    // not sure what U is supposed to mean\n    if (promptOptions.includeSiblingFunctions == SiblingOption.NoSiblings) {\n        U = curSrc.substring(0, offset);\n    }\n    // if sibling-functions option is enabled...\n    else {\n        const {\n            siblings: siblings,\n            beforeInsertion: beforeInsertion,\n            afterInsertion: afterInsertion,\n        } = await M_sibling_function_fetcher.getSiblingFunctions(curFile);\n\n        // appendLineForLine returns indices of the elements it added to the wishlist\n        pWishlist.appendLineForLine(\n            beforeInsertion,\n            M_prompt_choices_and_wishlist.PromptElementKind.BeforeCursor,\n            beforeCursorPriority\n        ).forEach((e) => beforeCursorElemIdxs.push(e));\n        \n        let siblingPriority = siblingsPriority;\n        siblings.forEach((e) => {\n            pWishlist.append(\n                e,\n                M_prompt_choices_and_wishlist.PromptElementKind.AfterCursor,\n                siblingPriority\n            );\n            // reduce priority for next sibling (coz siblings were sorted, i think by closeness to\n            // insertion point)\n            siblingPriority = priorities.justBelow(siblingPriority);\n        });\n        if (promptOptions.neighboringTabsPosition == NeighboringTabsPositionOption.AfterSiblings) {\n            addSnippetsToWishlist();\n        }\n        U = afterInsertion;\n    }\n\n    // dk who sets this option to DirectlyAboveCursor.\n    if (promptOptions.neighboringTabsPosition == NeighboringTabsPositionOption.DirectlyAboveCursor) {\n        const lastLineIdx = U.lastIndexOf(\"\\n\") + 1;\n        const textBeforeLastLine = U.substring(0, lastLineIdx);\n        const lastLineText = U.substring(lastLineIdx);\n        \n        // this is weird.....why would you wanna do this?\n        // include stuff in curfile till before the last line\n        // then add similar snippets and THEN add the last line?\n        // wouldn't it break the flow?\n        // i understand the last line needs to be at the end,\n        // because model needs to complete that line,\n        // but why break the current function...wat.\n        // this branch anyway appears to not be used so i guess things are fine\n        // but i might be wrong.\n        pWishlist.appendLineForLine(\n            textBeforeLastLine,\n            M_prompt_choices_and_wishlist.PromptElementKind.BeforeCursor,\n            beforeCursorPriority\n        ).forEach((e) => beforeCursorElemIdxs.push(e));\n        addSnippetsToWishlist();\n        \n        if (lastLineText.length > 0) {\n            // also, maybe i've named this variable incorrectly\n            // code above made it seem this only contained\n            // BeforeCursor elements. but here something else\n            // is going on.\n            beforeCursorElemIdxs.push(\n                pWishlist.append(\n                    lastLineText,\n                    M_prompt_choices_and_wishlist.PromptElementKind.AfterCursor,\n                    beforeCursorPriority\n                )\n            );\n            if (beforeCursorElemIdxs.length > 1) {\n                // lol inverse dependency? i really don't get what's happening when nbrTabsOption == DirectlyAboveCursor\n                pWishlist.require(beforeCursorElemIdxs[beforeCursorElemIdxs.length - 2], beforeCursorElemIdxs[beforeCursorElemIdxs.length - 1]);\n            }\n        }\n    } else\n        pWishlist.appendLineForLine(\n            U,\n            M_prompt_choices_and_wishlist.PromptElementKind.BeforeCursor,\n            beforeCursorPriority\n        ).forEach((e) => beforeCursorElemIdxs.push(e));\n    \n    if (LanguageMarkerOption.Top == promptOptions.languageMarker && beforeCursorElemIdxs.length > 0 && undefined !== langMarkerElemIdxs) {\n        // idk why this dependency exists.\n        pWishlist.require(langMarkerElemIdxs, beforeCursorElemIdxs[0]);\n    }\n    if (PathMarkerOption.Top == promptOptions.pathMarker && beforeCursorElemIdxs.length > 0 && undefined !== pathMarkerElemIdxs) {\n        // again. why does this dependency exist?\n        if (langMarkerElemIdxs) {\n            pWishlist.require(pathMarkerElemIdxs, langMarkerElemIdxs);\n        } else {\n            pWishlist.require(pathMarkerElemIdxs, beforeCursorElemIdxs[0]);\n        }\n    }\n\n    if (undefined !== langMarkerElemIdxs && undefined !== pathMarkerElemIdxs) {\n        // um. why this anti-dependency? i don't get it. doesn't this contradict the\n        // above dependency?\n        pWishlist.exclude(pathMarkerElemIdxs, langMarkerElemIdxs);\n    }\n\n    let suffix = curSrc.slice(offset);\n    // oh wow, suffix length threshold is a LOWER bound. i thought it was an upper bound.\n    if (0 == promptOptions.suffixPercent || suffix.length <= promptOptions.fimSuffixLengthThreshold)\n        return pWishlist.fulfill(promptOptions.maxPromptLength);\n    \n    // a random block. why not.\n    // this stuff deals with suffix.\n    {\n        let suffixOffset = curFile.offset;\n        if (\n            promptOptions.suffixStartMode !== SuffixStartMode.Cursor &&\n            promptOptions.suffixStartMode !== SuffixStartMode.CursorTrimStart\n        ) {\n            // this function appears to find where the next sibling function starts\n            // AFTER the cursor (if no sibling function is found, it returns cursor offset)\n            suffixOffset = await M_sibling_function_fetcher.getSiblingFunctionStart(curFile);\n        }\n\n        const budget = promptOptions.maxPromptLength - exports.TOKENS_RESERVED_FOR_SUFFIX_ENCODING;\n\n        let prefixBudget = Math.floor((budget * (100 - promptOptions.suffixPercent)) / 100);\n        let fulfilment = pWishlist.fulfill(prefixBudget);\n        // suffixBudget may be greater than budget * suffixPercent / 100\n        // because prefixBudget needn't be fully used.\n        // i think.\n        const suffixBudget = budget - fulfilment.prefixLength;\n        \n        let suffixText = curSrc.slice(suffixOffset);\n        if (\n            promptOptions.suffixStartMode != SuffixStartMode.SiblingBlockTrimStart &&\n            promptOptions.suffixStartMode != SuffixStartMode.CursorTrimStart\n        ) {\n            suffixText = suffixText.trimStart();\n        }\n\n        const suffixTokens = M_tokenizer.takeFirstTokens(suffixText, suffixBudget);\n        if (suffixTokens.tokens.length <= suffixBudget - 3) {\n            // what's this 3?\n            prefixBudget = budget - suffixTokens.tokens.length;\n            // oh if suffix tokens are less than the suffix budget,\n            // we can expand the prefix budget.\n            // okay, nice you're greedy, I like that.\n            fulfilment = pWishlist.fulfill(prefixBudget);\n        }\n\n        // SuffixMatchOption.Equal means the cached suffix is used\n        // if the suffix is EXACTLY equal to the cached suffix.\n        if (promptOptions.suffixMatchCriteria == SuffixMatchOption.Equal) {\n            if (\n                suffixTokens.tokens.length === cachedSuffix.tokens.length &&\n                suffixTokens.tokens.every((e, t) => e === cachedSuffix.tokens[t])\n            ) {\n                useCachedSuffix = true;\n            }\n        } else {\n            // this is the levenshtein distance stuff.\n            // also, damn, what a long if statement.\n            if (\n                promptOptions.suffixMatchCriteria == SuffixMatchOption.Levenshtein &&\n                suffixTokens.tokens.length > 0 &&\n                promptOptions.suffixMatchThreshold > 0 &&\n                100 *\n                (null ===\n                    (suffixVsCachedSuffixEditDist = M_edit_distance.findEditDistanceScore(\n                        suffixTokens.tokens.slice(0, exports.MAX_EDIT_DISTANCE_LENGTH),\n                        cachedSuffix.tokens.slice(0, exports.MAX_EDIT_DISTANCE_LENGTH)\n                    )) || undefined === suffixVsCachedSuffixEditDist\n                    ? undefined\n                    : suffixVsCachedSuffixEditDist.score) <\n                promptOptions.suffixMatchThreshold *\n                Math.min(exports.MAX_EDIT_DISTANCE_LENGTH, suffixTokens.tokens.length)\n            ) {\n                useCachedSuffix = true;\n            }\n        }\n\n        if (true === useCachedSuffix && cachedSuffix.tokens.length <= suffixBudget) {\n            if (cachedSuffix.tokens.length <= suffixBudget - 3) {\n                // again, recompute prefix budget (in case suffix budget is greater than\n                // what's required by suffix)\n                prefixBudget = budget - cachedSuffix.tokens.length;\n                fulfilment = pWishlist.fulfill(prefixBudget);\n            }\n            fulfilment.suffix = cachedSuffix.text;\n            fulfilment.suffixLength = cachedSuffix.tokens.length;\n        } else {\n            fulfilment.suffix = suffixTokens.text;\n            fulfilment.suffixLength = suffixTokens.tokens.length;\n            cachedSuffix = suffixTokens;\n        }\n        return fulfilment;\n    }\n};\n",
    "3055456": "// prompt-choices-and-wishlist.js\nObject.defineProperty(exports, \"__esModule\", {\n  value: !0,\n});\nexports.Priorities =\n  exports.PromptWishlist =\n  exports.PromptElementRanges =\n  exports.PromptChoices =\n  exports.PromptBackground =\n  exports.PromptElementKind =\n  undefined;\nconst M_get_prompt_actual = require(\"get-prompt-actual\");\nconst M_tokenizer = require(\"tokenizer\");\nvar i;\n!(function (e) {\n  e.BeforeCursor = \"BeforeCursor\";\n  e.AfterCursor = \"AfterCursor\";\n  e.SimilarFile = \"SimilarFile\";\n  e.ImportedFile = \"ImportedFile\";\n  e.LanguageMarker = \"LanguageMarker\";\n  e.PathMarker = \"PathMarker\";\n})((i = exports.PromptElementKind || (exports.PromptElementKind = {})));\nclass PromptBackground {\n  constructor() {\n    this.used = new Map();\n    this.unused = new Map();\n  }\n  markUsed(e) {\n    if (this.IsNeighboringTab(e)) {\n      this.used.set(e.id, this.convert(e));\n    }\n  }\n  undoMarkUsed(e) {\n    if (this.IsNeighboringTab(e)) {\n      this.used.delete(e.id);\n    }\n  }\n  markUnused(e) {\n    if (this.IsNeighboringTab(e)) {\n      this.unused.set(e.id, this.convert(e));\n    }\n  }\n  convert(e) {\n    return {\n      score: e.score.toFixed(4),\n      length: e.text.length,\n    };\n  }\n  IsNeighboringTab(e) {\n    return e.kind == i.SimilarFile;\n  }\n}\nexports.PromptBackground = PromptBackground;\nclass PromptChoices {\n  constructor() {\n    this.used = new Map();\n    this.unused = new Map();\n  }\n  markUsed(e) {\n    this.used.set(e.kind, (this.used.get(e.kind) || 0) + e.tokens);\n  }\n  undoMarkUsed(e) {\n    this.used.set(e.kind, (this.used.get(e.kind) || 0) - e.tokens);\n  }\n  markUnused(e) {\n    this.unused.set(e.kind, (this.used.get(e.kind) || 0) + e.tokens);\n  }\n}\nexports.PromptChoices = PromptChoices;\nclass PromptElementRanges {\n  constructor(elems) {\n    this.ranges = new Array();\n    let lastKind;\n    let n = 0;\n    for (const { element: elem } of elems)\n      if (0 !== elem.text.length) {\n        if (lastKind === i.BeforeCursor && elem.kind === i.BeforeCursor) {\n          // merge adjacent BeforeCursor elements\n          this.ranges[this.ranges.length - 1].end += elem.text.length;\n        } else {\n          // add a new range otherwise\n          this.ranges.push({\n            kind: elem.kind,\n            start: n,\n            end: n + elem.text.length,\n          });\n        }\n        lastKind = elem.kind;\n        n += elem.text.length;\n      }\n  }\n}\nexports.PromptElementRanges = PromptElementRanges;\nexports.PromptWishlist = class {\n  constructor(e) {\n    this.content = [];\n    this.lineEndingOption = e;\n  }\n  \n  getContent() {\n    return [...this.content];\n  }\n\n  convertLineEndings(text) {\n    if (\n      this.lineEndingOption ===\n      M_get_prompt_actual.LineEndingOptions.ConvertToUnix\n    ) {\n      text = text.replace(/\\r\\n/g, \"\\n\").replace(/\\r/g, \"\\n\");\n    }\n    return text;\n  }\n\n  append(text, kind, priority, nTokens = M_tokenizer.tokenLength(text), score = NaN) {\n    text = this.convertLineEndings(text);\n    const idx = this.content.length;\n    this.content.push({\n      id: idx,\n      text: text,\n      kind: kind,\n      priority: priority,\n      tokens: nTokens,\n      requires: [],\n      excludes: [],\n      score: score,\n    });\n    return idx;\n  }\n\n  appendLineForLine(text, kind, priority) {\n    const lines = (text = this.convertLineEndings(text)).split(\"\\n\");\n    for (let i = 0; i < lines.length - 1; i++) lines[i] += \"\\n\";\n\n    // merge double newlines\n    const lines2 = [];\n    lines.forEach((line, i) => {\n      if (\"\\n\" === line && lines2.length > 0 && !lines2[lines2.length - 1].endsWith(\"\\n\\n\")) {\n        lines2[lines2.length - 1] += \"\\n\";\n      } else {\n        lines2.push(line);\n      }\n    });\n\n    const insertIdxs = [];\n    lines2.forEach((line, lineNo) => {\n      if (\"\" !== line) {\n        insertIdxs.push(this.append(line, kind, priority));\n        if (lineNo > 0) {\n          // require the previous line\n          this.content[this.content.length - 2].requires = [\n            this.content[this.content.length - 1],\n          ];\n        }\n      }\n    });\n\n    return insertIdxs;\n  }\n\n  require(idx1, idx2) {\n    const el1 = this.content.find((t) => t.id === idx1);\n    const el2 = this.content.find((e) => e.id === idx2);\n    if (el1 && el2) {\n      el1.requires.push(el2);\n    }\n  }\n\n  exclude(idx1, idx2) {\n    const el1 = this.content.find((t) => t.id === idx1);\n    const el2 = this.content.find((e) => e.id === idx2);\n    if (el1 && el2) {\n      el1.excludes.push(el2);\n    }\n  }\n\n  fulfill(tokenBudget) {\n    // fulfill the wishlist given a token budget\n\n    const promptChoices = new PromptChoices();\n    const promptBackground = new PromptBackground();\n    const wishlist = this.content.map((elem, idx) => ({\n      element: elem,\n      index: idx,\n    }));\n\n    // sort by (priority, index)\n    wishlist.sort((e, t) =>\n      e.element.priority === t.element.priority\n        ? t.index - e.index\n        : t.element.priority - e.element.priority\n    );\n\n    // sets of included and excluded indices\n    const included = new Set();\n    const excluded = new Set();\n    let firstOutOfBudgetElem; // the first element that is out of budget\n                              // keeping track of this because:\n                              // sum(len(elem.tokens) for elem in elems) >= sum(len(tokenize(cat(elem.text for elem in elems)))\n                              // and the loop checks number of consumed tokens using the equation on the left,\n                              // which is an overestimate of actual number of tokens (on the right)\n                              // so we remember which was the almost-included element,\n                              // concat all text of included and this element, see if we're under budget\n                              // if so, we can get a tiny bit of more information in the prompt.\n                              // i really don't know how much benefit this can provide,\n                              // but interesting to think about. lol.\n    const addedElems = []; // same as included, but sorted, and with elements, i.e., [{element, index}, ...]\n    let availableTokens = tokenBudget;\n\n    wishlist.forEach((elemWithIdx) => {\n      var followingElemWithIdx;\n      const elem = elemWithIdx.element;\n      const idx = elemWithIdx.index;\n\n      if (\n        availableTokens >= 0 &&\n        (availableTokens > 0 || undefined === firstOutOfBudgetElem) &&\n        elem.requires.every((e) => included.has(e.id)) &&\n        !excluded.has(elem.id)\n      ) {\n        let nTokens = elem.tokens;\n        // oof, crazy obfuscated code\n        const followingElem =\n          null ===\n            (followingElemWithIdx = (function (addedElems, idx) {\n              let followingElemWithIdx;\n              let minIdx = 1 / 0;\n              for (const addedElem of addedElems)\n                if (addedElem.index > idx && addedElem.index < minIdx) {\n                  followingElemWithIdx = addedElem;\n                  minIdx = addedElem.index;\n                }\n              return followingElemWithIdx;\n            })(addedElems, idx)) || undefined === followingElemWithIdx\n            ? undefined\n            : followingElemWithIdx.element;\n        \n        // some edge case handling about token computation and whitespaces.\n        if (elem.text.endsWith(\"\\n\\n\") && followingElem && !followingElem.text.match(/^\\s/)) {\n          nTokens++;\n        }\n\n        if (availableTokens >= nTokens) {\n          availableTokens -= nTokens;\n          included.add(elem.id);\n          elem.excludes.forEach((e) => excluded.add(e.id));\n          promptChoices.markUsed(elem);\n          promptBackground.markUsed(elem);\n          addedElems.push(elemWithIdx);\n        } else {\n          firstOutOfBudgetElem = null != firstOutOfBudgetElem ? firstOutOfBudgetElem : elemWithIdx;\n        }\n      } else {\n        promptChoices.markUnused(elem);\n        promptBackground.markUnused(elem);\n      }\n    });\n\n    // even though we process elements by priority, the prompt is generated\n    // after sorting by index, so that the order of elements is preserved\n    addedElems.sort((e, t) => e.index - t.index);\n    \n    let prefix = addedElems.reduce((result, cur) => result + cur.element.text, \"\");\n    let prefixLen = M_tokenizer.tokenLength(prefix);\n    \n    // while we're exceeding the token budget, remove the last element\n    for (; prefixLen > tokenBudget;) {\n      // sort by (priority, index)\n      addedElems.sort((e, t) =>\n        t.element.priority === e.element.priority\n          ? t.index - e.index\n          : t.element.priority - e.element.priority\n      );\n\n      const lastImpElem = addedElems.pop();\n      if (lastImpElem) {\n        // if we remove this, shouldn't we also respect its requires and excludes?\n        // doesn't look like that's happening here\n        promptChoices.undoMarkUsed(lastImpElem.element);\n        promptChoices.markUnused(lastImpElem.element);\n        promptBackground.undoMarkUsed(lastImpElem.element);\n        promptBackground.markUnused(lastImpElem.element);\n        firstOutOfBudgetElem = undefined;\n      }\n\n      addedElems.sort((e, t) => e.index - t.index);\n      prefix = addedElems.reduce((e, t) => e + t.element.text, \"\");\n      prefixLen = M_tokenizer.tokenLength(prefix);\n    }\n\n    // why do we need to copy this?\n    const addedElemsCopy = [...addedElems];\n    \n    // if we almost included an element, but didn't because we ran out of budget,\n    // recompute the number of tokens more precisely\n    if (undefined !== firstOutOfBudgetElem) {\n      addedElemsCopy.push(firstOutOfBudgetElem);\n      addedElemsCopy.sort((e, t) => e.index - t.index);\n      \n      const prefix = addedElemsCopy.reduce((e, t) => e + t.element.text, \"\");\n      const prefixLen = M_tokenizer.tokenLength(prefix);\n      \n      if (prefixLen <= tokenBudget) {\n        // yay, we squeezed in a tiny bit more information\n        promptChoices.markUsed(firstOutOfBudgetElem.element);\n        promptBackground.markUsed(firstOutOfBudgetElem.element);\n        const promptElemRngs = new PromptElementRanges(addedElemsCopy);\n        \n        return {\n          prefix: prefix,\n          suffix: \"\",\n          prefixLength: prefixLen,\n          suffixLength: 0,\n          promptChoices: promptChoices,\n          promptBackground: promptBackground,\n          promptElementRanges: promptElemRngs,\n        };\n      }\n      promptChoices.markUnused(firstOutOfBudgetElem.element);\n      promptBackground.markUnused(firstOutOfBudgetElem.element);\n    }\n\n    const promptElemRngs = new PromptElementRanges(addedElems);\n    return {\n      prefix: prefix,\n      suffix: \"\",\n      prefixLength: prefixLen,\n      suffixLength: 0,\n      promptChoices: promptChoices,\n      promptBackground: promptBackground,\n      promptElementRanges: promptElemRngs,\n    };\n  }\n};\n\nclass Priorities {\n  constructor() {\n    this.registeredPriorities = [0, 1];\n  }\n  register(e) {\n    if (e > Priorities.TOP || e < Priorities.BOTTOM)\n      throw new Error(\"Priority must be between 0 and 1\");\n    this.registeredPriorities.push(e);\n    return e;\n  }\n  justAbove(...e) {\n    const t = Math.max(...e);\n    const n = Math.min(...this.registeredPriorities.filter((e) => e > t));\n    return this.register((n + t) / 2);\n  }\n  justBelow(...e) {\n    const t = Math.min(...e);\n    const n = Math.max(...this.registeredPriorities.filter((e) => e < t));\n    return this.register((n + t) / 2);\n  }\n  between(e, t) {\n    if (\n      this.registeredPriorities.some((n) => n > e && n < t) ||\n      !this.registeredPriorities.includes(e) ||\n      !this.registeredPriorities.includes(t)\n    )\n      throw new Error(\"Priorities must be adjacent in the list of priorities\");\n    return this.register((e + t) / 2);\n  }\n}\nexports.Priorities = Priorities;\nPriorities.TOP = 1;\nPriorities.BOTTOM = 0;\n",
    "3055670": "// sibling-function-fetcher\nObject.defineProperty(exports, \"__esModule\", {\n  value: !0,\n});\nexports.getSiblingFunctionStart = exports.getSiblingFunctions = undefined;\n\nconst M_get_prompt_actual = require(\"get-prompt-actual\");\nconst M_get_prompt_parsing_utils = require(\"get-prompt-parsing-utils\");\n\nexports.getSiblingFunctions = async function ({\n  source: source,\n  offset: offset,\n  languageId: languageId,\n}) {\n  var i;\n  var s;\n  const siblings = [];\n  let beforeInsertion = \"\";\n  let afterInsertion = source.substring(0, offset);\n\n  if (M_get_prompt_parsing_utils.isSupportedLanguageId(languageId)) {\n    const parseTree = await M_get_prompt_parsing_utils.parseTree(languageId, source);\n    try {\n      let idx = offset;\n      for (; idx >= 0 && /\\s/.test(source[idx]);)\n        idx--;\n      const curDescendent = parseTree.rootNode.descendantForIndex(idx);\n      const ancestor = M_get_prompt_parsing_utils.getAncestorWithSiblingFunctions(\n        languageId,\n        curDescendent\n      );\n      if (ancestor) {\n        const firstComment = M_get_prompt_parsing_utils.getFirstPrecedingComment(ancestor);\n        // either firstComment.startIndex or ancestor.startIndex\n        const startIdx =\n          null !== (i = null == firstComment ? undefined : firstComment.startIndex) && undefined !== i\n            ? i\n            : ancestor.startIndex;\n        let p;\n        let f = 0;\n        for (; \" \" == (p = source[startIdx - f - 1]) || \"\\t\" == p;) f++;\n        const ws = source.substring(startIdx - f, startIdx); // whitespace (i think)\n\n        for (let sibling = ancestor.nextSibling; sibling; sibling = sibling.nextSibling)\n          if (M_get_prompt_parsing_utils.isFunctionDefinition(languageId, sibling)) {\n            const comment = M_get_prompt_parsing_utils.getFirstPrecedingComment(sibling);\n            // either comment.startIndex or sibling.startIndex\n            const startIdx =\n              null !== (s = null == comment ? undefined : comment.startIndex) &&\n                undefined !== s\n                ? s\n                : sibling.startIndex;\n            // what. wouldn't startIdx always be less than offset?\n            if (startIdx < offset) continue;\n            const siblingSrc = source.substring(startIdx, sibling.endIndex);\n            const siblingSrc2 = M_get_prompt_actual.newLineEnded(siblingSrc) + \"\\n\" + ws;\n            siblings.push(siblingSrc2);\n          }\n        beforeInsertion = source.substring(0, startIdx);\n        afterInsertion = source.substring(startIdx, offset);\n      }\n    } finally {\n      parseTree.delete();\n    }\n  }\n\n  return {\n    siblings: siblings,\n    beforeInsertion: beforeInsertion,\n    afterInsertion: afterInsertion,\n  };\n};\nexports.getSiblingFunctionStart = async function ({\n  source: source,\n  offset: offset,\n  languageId: languageId,\n}) {\n  var r;\n  if (M_get_prompt_parsing_utils.isSupportedLanguageId(languageId)) {\n    const parseTree = await M_get_prompt_parsing_utils.parseTree(languageId, source);\n    try {\n      let idx = offset;\n      for (; idx >= 0 && /\\s/.test(source[idx]);)\n        idx--;\n\n      const desc = parseTree.rootNode.descendantForIndex(idx);\n      const ancestor = M_get_prompt_parsing_utils.getAncestorWithSiblingFunctions(\n        languageId,\n        desc\n      );\n\n      if (ancestor) {\n        for (let sibling = ancestor.nextSibling; sibling; sibling = sibling.nextSibling)\n          if (M_get_prompt_parsing_utils.isFunctionDefinition(languageId, sibling)) {\n            const comment = M_get_prompt_parsing_utils.getFirstPrecedingComment(sibling);\n            const startIdx =\n              null !== (r = null == comment ? undefined : comment.startIndex) &&\n                undefined !== r\n                ? r\n                : sibling.startIndex;\n            if (startIdx < offset) continue;\n            return startIdx;\n          }\n        if (ancestor.endIndex >= offset)\n          return ancestor.endIndex;\n      }\n    } finally {\n      parseTree.delete();\n    }\n  }\n  return offset;\n};\n"
}